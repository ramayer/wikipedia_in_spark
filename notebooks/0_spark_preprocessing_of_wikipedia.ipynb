{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "parental-likelihood",
   "metadata": {},
   "source": [
    "# Preprocess Wikipedia with Spark\n",
    "\n",
    "## TL/DR:\n",
    "\n",
    "1. Start with a Wikipedia Dump\n",
    "2. Process the data in Spark.\n",
    "3. Produce maps like this: ![alt text](all_wikipedia_coords.png \"Title\")\n",
    "\n",
    "Resources:\n",
    "* https://en.wikipedia.org/wiki/Wikipedia:Database_download\n",
    "* https://towardsdatascience.com/wikipedia-data-science-working-with-the-worlds-largest-encyclopedia-c08efbac5f5c\n",
    "* https://github.com/earwig/mwparserfromhell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "middle-incentive",
   "metadata": {},
   "source": [
    "## Install dependencies\n",
    "\n",
    "This is using `%pip` rather than `pkg_rsources.resolve()` because on databricks clusters, `%pip` will make sure the libraries are available on the spark worker nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raised-shock",
   "metadata": {},
   "outputs": [],
   "source": [
    "required_packages = {\"mwparserfromhell\",\"geopandas\",\"h3\",\"geocoder\",\"pydeck\"}\n",
    "\n",
    "import pkg_resources\n",
    "for lib in required_packages - {pkg.key for pkg in pkg_resources.working_set}:\n",
    "    print(f\"installing {lib}\")\n",
    "    %pip install -q --upgrade pip\n",
    "    %pip install -q $lib\n",
    "    pkg_resources.require(lib)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "built-tuesday",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to find (and if needed download) a wikipedia dump file\n",
    "source_dir      = './data/wikipedia_src'\n",
    "source_dump     = source_dir + '/enwiki-latest-pages-articles-multistream.xml.bz2'\n",
    "\n",
    "# Where to store temporary pre-processed files before creating spark tables.  Will temporarily use ~70GB.\n",
    "preprocess_dir  = './data/wikipedia_as_json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "demanding-dominant",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.sax\n",
    "import json\n",
    "import mwparserfromhell\n",
    "import subprocess\n",
    "import json\n",
    "import time\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protected-consumer",
   "metadata": {},
   "source": [
    "## Fetch the latest Wikipedia dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expanded-qualification",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p data/wikipedia_src data/wikipedia_as_json\n",
    "! (cd data/wikipedia_src; wget -nc https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles-multistream.xml.bz2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tough-hundred",
   "metadata": {},
   "source": [
    "## Pre-process the Wikimedia XML to JSONL / NLJSON\n",
    "\n",
    "* Spark's tooling for JSONL is better than it's XML handling so pre-process the XML to JSONL.\n",
    "\n",
    "* Put 100,000 Wikipeda pages in each .json file.\n",
    "    * JSON files each containing 10,000 to 100,000 Wikipedia pages seems to be a sweet spot for loading as Spark dataframes on this cluster.\n",
    "    * This generates about 215 files, averaging 325MB each.\n",
    "\n",
    "* Yes, this preprocessing step could be further optimized, but it doesn't matter.\n",
    "    * The XML->JSON conversion is fast enough it doesn't need to be parallized. It takes under 2 hours on a single CPU core.  In contrast, parsing the wikimedia markup below takes many hours on a cluster of 32 CPUs.  Compressing the temp files could save space, but it doesn't matter either.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "spiritual-turner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based heavily on\n",
    "# https://github.com/WillKoehrsen/wikipedia-data-science/blob/master/notebooks/Downloading%20and%20Parsing%20Wikipedia%20Articles.ipynb\n",
    "\n",
    "class WikiXmlHandler(xml.sax.handler.ContentHandler):\n",
    "    \"\"\"Content handler for Wiki XML data using SAX\"\"\"\n",
    "    def __init__(self):\n",
    "        xml.sax.handler.ContentHandler.__init__(self)\n",
    "        self._buffer = None\n",
    "        self._values = {}\n",
    "        self._current_tag = None\n",
    "        self._pages = []\n",
    "        self._page_count = 0\n",
    "\n",
    "    def characters(self, content):\n",
    "        \"\"\"Characters between opening and closing tags\"\"\"\n",
    "        if self._current_tag:\n",
    "            self._buffer.append(content)\n",
    "\n",
    "    def startElement(self, name, attrs):\n",
    "        \"\"\"Opening tag of element\"\"\"\n",
    "        if name in ('title', 'text', 'timestamp'):\n",
    "            self._current_tag = name\n",
    "            self._buffer = []\n",
    "\n",
    "    def endElement(self, name):\n",
    "        \"\"\"Closing tag of element\"\"\"\n",
    "        if name == self._current_tag:\n",
    "            self._values[name] = ' '.join(self._buffer)\n",
    "        if name == 'page':\n",
    "            self._pages.append((self._values['title'], self._values['text']))\n",
    "            \n",
    "    def save_partial_results(self,path):\n",
    "        self._page_count += len(handler._pages)\n",
    "        filename = f'{path}/{self._page_count}.jsonl'\n",
    "        with open(filename,'w') as jsonfile:\n",
    "            for page in handler._pages:\n",
    "                page_dict = {'title':page[0],'body':page[1]}\n",
    "                json.dump(page_dict,jsonfile)\n",
    "                jsonfile.write(\"\\n\")\n",
    "        self._pages = []\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "presidential-spectacular",
   "metadata": {},
   "outputs": [],
   "source": [
    "rebuild_json_files = False\n",
    "if rebuild_json_files:\n",
    "    wikipedia_dump = source_dump\n",
    "    handler        = WikiXmlHandler()\n",
    "    parser         = xml.sax.make_parser()\n",
    "    parser.setContentHandler(handler)\n",
    "    expected_pages = 21100000  # Doesn't need to be exact. Just used to show progress.\n",
    "    t0 = time.time()\n",
    "    for i, line in enumerate(subprocess.Popen(['bzcat'], \n",
    "                             stdin = open(wikipedia_dump), \n",
    "                             stdout = subprocess.PIPE).stdout):\n",
    "        parser.feed(line)\n",
    "        if len(handler._pages) >= 100000:\n",
    "            handler.save_partial_results(preprocess_dir)\n",
    "            t1 = time.time()\n",
    "            et = t1 - t0\n",
    "            page_count = handler._page_count\n",
    "            print(f'page {page_count} at {int(et)} seconds '\n",
    "                  f'({int(page_count/(et))} per second). '\n",
    "                  f'Estimate {int(et*expected_pages/page_count/60)} minutes '\n",
    "                  f'{et*(expected_pages-page_count)/page_count/60} remaining'\n",
    "                 )\n",
    "    handler.save_partial_results(preprocess_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unique-distribution",
   "metadata": {},
   "source": [
    "## Launch Spark (if running on a standalone environment)\n",
    "\n",
    "* On databricks clusters the Spark Context will already have existed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "raised-development",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://08651cc65d10:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>MyApp</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ff9cf6f6e50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not \"spark\" in locals():\n",
    "    import pyspark\n",
    "    MAX_MEMORY = \"8g\"  # 24 gives OOM here. # 6 gives \"out of heap space\"\n",
    "    spark = (pyspark.sql.SparkSession.builder.appName(\"MyApp\") \n",
    "        .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:0.8.0\") \n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \n",
    "        .config(\"spark.executor.memory\", MAX_MEMORY) \n",
    "        .config(\"spark.driver.memory\", MAX_MEMORY) \n",
    "        .config(\"spark.python.worker.reuse\",False)\n",
    "        .config(\"spark.task.maxFailures\",5)\n",
    "        .enableHiveSupport() \n",
    "        .getOrCreate()        \n",
    "        )\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "polyphonic-briefing",
   "metadata": {},
   "outputs": [],
   "source": [
    "rebuild_delta_table = False\n",
    "if rebuild_delta_table:\n",
    "    t0 = time.perf_counter()\n",
    "    df = spark.read.json(preprocess_dir,'title string, body string')\n",
    "    df.select('title','body').write.format('delta').saveAsTable('wikipedia_bronze')\n",
    "    print(f\"saved as delta table in {time.perf_counter() - t0} seconds\")\n",
    "    napier_df = spark.sql(\"select * from wikipedia_bronze where title = 'Charles James Napier' limit 3\").cache()\n",
    "    napier_df.write.format('delta').saveAsTable('wikipedia_bronze_small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "material-moral",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+---------------------------------------------+\n",
      "|title                        |body                                         |\n",
      "+-----------------------------+---------------------------------------------+\n",
      "|Black panther                |{{short description|Melanistic colour va...  |\n",
      "|Great white shark            |{{other uses of|great white|Great White ...  |\n",
      "|Melisende, Queen of Jerusalem|{{short description|Queen regnant of the...  |\n",
      "|Carcharodon carcharias       |#REDIRECT [[Great white shark]] \\n \\n {{Re...|\n",
      "|Method                       |{{Wiktionary|method}} \\n '''Method''' ({{... |\n",
      "+-----------------------------+---------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(r\"select title,replace(substr(body,1,40)||'...','\\n','\\\\n') as body from wikipedia_bronze limit 5\").show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proud-paradise",
   "metadata": {},
   "source": [
    "## Parse the wikipedia content\n",
    "\n",
    "* Wraps the excellent but pretty slow `mwparserfromhell` library: https://github.com/earwig/mwparserfromhell\n",
    "\n",
    "* Attempts to have this Pandas UDF return comples structures like \n",
    "\n",
    "      infoboxes array<struct<name:string,params:map<string,string>,body:string>>,\n",
    "      templates array<struct<name:string,params:map<string,string>,body:string>>,\n",
    "      extlinks  array<struct<title:string,url:string>>,\n",
    "      wikilinks array<struct<text:string,title:string>>,\n",
    "      _error_   string\n",
    "      \n",
    "    are failing with the following in Spark 3.1.1:\n",
    "\n",
    "        21/04/15 07:05:29 WARN BlockManager: Putting block rdd_66_0 failed due to exception org.apache.spark.TaskKilledException.\n",
    "        21/04/15 07:05:29 ERROR TaskContextImpl: Error in TaskCompletionListener\n",
    "        java.lang.IllegalStateException: Memory was leaked by query. Memory leaked: (4194304)\n",
    "        Allocator(stdin reader for python3) 0/4194304/12602208/9223372036854775807 (res/actual/peak/limit)\n",
    "        at org.apache.arrow.memory.BaseAllocator.close(BaseAllocator.java:431)\n",
    "    \n",
    "* Work around it by returning the data encoded as a json string.    \n",
    "  Doesn't add much overhead compared to the actual parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "excess-realtor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as psf\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import h3 as h3\n",
    "\n",
    "def all_h3_grids(lat,lon):\n",
    "    return([h3.geo_to_h3(lat, lon, lvl)  for lvl in range(16)])\n",
    "\n",
    "# https://en.wikipedia.org/wiki/Template:Coord\n",
    "#\n",
    "# {{coord|latitude|longitude|coordinate parameters|template parameters}}\n",
    "# {{coord|dd|N/S|dd|E/W|coordinate parameters|template parameters}}\n",
    "# {{coord|dd|mm|N/S|dd|mm|E/W|coordinate parameters|template parameters}}\n",
    "# {{coord|dd|mm|ss|N/S|dd|mm|ss|E/W|coordinate parameters|template parameters}}\n",
    "#\n",
    "def interpret_wikimedia_coordinate(parsed_template):\n",
    "    lat = lon = None\n",
    "    params = {p.name.strip(): p.value.strip() for p in parsed_template.params}\n",
    "    if not (params.get('4') in ['N','S']\n",
    "            or \n",
    "            params.get('3') in ['N','S']\n",
    "            or \n",
    "            params.get('2') in ['N','S']\n",
    "           ):\n",
    "        lat = float(params['1'])\n",
    "        lon = float(params['2'])\n",
    "    if params.get('4') == 'N': lat =    int(params['1']) + float(params['2'])/60 + float(params['3'])/60/60\n",
    "    if params.get('4') == 'S': lat = - (int(params['1']) + float(params['2'])/60 + float(params['3'])/60/60)\n",
    "    if params.get('8') == 'E': lon =    int(params['5']) + float(params['6'])/60 + float(params['7'])/60/60\n",
    "    if params.get('8') == 'W': lon = - (int(params['5']) + float(params['6'])/60 + float(params['7'])/60/60)\n",
    "    if params.get('3') == 'N': lat =    int(params['1']) + float(params['2'])/60 \n",
    "    if params.get('3') == 'S': lat = - (int(params['1']) + float(params['2'])/60 )\n",
    "    if params.get('6') == 'E': lon =    int(params['4']) + float(params['5'])/60 \n",
    "    if params.get('6') == 'W': lon = - (int(params['4']) + float(params['5'])/60 )\n",
    "    if params.get('2') == 'N': lat =    float(params['1'])\n",
    "    if params.get('2') == 'S': lat = - (float(params['1']) )\n",
    "    if params.get('4') == 'E': lon =    float(params['3'])\n",
    "    if params.get('4') == 'W': lon = - (float(params['3']) )\n",
    "    if lat and lon: return [lat,lon]\n",
    "    return None\n",
    "\n",
    "# https://en.wikipedia.org/wiki/Template:Birth_date\n",
    "def extract_mediawiki_info(wikimedia_text):\n",
    "    def nstr(s): \n",
    "        return(s and s.strip())\n",
    "    def params_as_dict(tmpl):\n",
    "        return({p.name.strip(): p.value.strip() for p in tmpl.params})\n",
    "    def tmpl_as_dict(tmpl):\n",
    "        return({\"name\":nstr(tmpl.name),\"params\":params_as_dict(tmpl),\"body\":nstr(tmpl)})\n",
    "    try:\n",
    "        p = mwparserfromhell.parse(wikimedia_text)\n",
    "        all_tmpl    = p.filter_templates(recursive=True) \n",
    "        top_tmpl    = p.filter_templates(recursive=False)\n",
    "        raw_coords  = [t for t in all_tmpl if t.name.strip().lower() == 'coord']\n",
    "        lat_lon = [interpret_wikimedia_coordinate(coord) for coord in raw_coords if coord]\n",
    "        coords  = [{'lat':coord[0],\n",
    "                    'lon':coord[1],\n",
    "                    'h3':all_h3_grids(coord[0],coord[1])\n",
    "                  } for coord in lat_lon]\n",
    "        templates   = [tmpl_as_dict(t) for t in top_tmpl]\n",
    "        infoboxes   = [t for t in templates if     t['name'].lower().startswith('infobox') ]\n",
    "        othertmpl   = [t for t in templates if not t['name'].lower().startswith('infobox') ]\n",
    "        wikilinks   = [{\"title\":nstr(l.title),\"text\":nstr(l.text)} for l in p.filter_wikilinks()]\n",
    "        extlinks    = [{\"title\":nstr(l.title),\"url\":nstr(l.url)}   for l in p.filter_external_links()]\n",
    "        return json.dumps({\n",
    "            \"infoboxes\":infoboxes,\n",
    "            \"coords\":coords,\n",
    "            \"templates\":othertmpl,\n",
    "            \"wikilinks\":wikilinks,\n",
    "            \"extlinks\":extlinks\n",
    "        })\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"_error_\":str(e)})\n",
    "\n",
    "parsed_schema = '''\n",
    "      infoboxes array<struct<name:string,params:map<string,string>,body:string>>,\n",
    "      templates array<struct<name:string,params:map<string,string>,body:string>>,\n",
    "      extlinks  array<struct<title:string,url:string>>,\n",
    "      wikilinks array<struct<text:string,title:string>>,\n",
    "      coords    array<struct<lat:float,lon:float,h3:array<string>>>,\n",
    "      _error_   string\n",
    "    '''\n",
    "\n",
    "@psf.pandas_udf('string')\n",
    "def extract_mediawiki_info_udf(s: pd.Series) -> pd.Series:\n",
    "    return s.apply(extract_mediawiki_info)\n",
    "spark.udf.register('extract_mediawiki_info_udf',extract_mediawiki_info_udf)\n",
    "\n",
    "def template_params(wikimedia_text):\n",
    "    try:\n",
    "        p = mwparserfromhell.parse(wikimedia_text)\n",
    "        templates   = p.filter_templates(recursive=False)\n",
    "        if len(templates) == 0: return None\n",
    "        params = {p.name.strip(): p.value.strip() for p in templates[0].params}\n",
    "        return params\n",
    "    except Exception as e:\n",
    "        return {}\n",
    "\n",
    "@psf.pandas_udf('map<string,string>')\n",
    "def template_params_udf(s: pd.Series) -> pd.Series:\n",
    "    return s.apply(template_params)\n",
    "spark.udf.register('template_params_udf',template_params_udf)\n",
    "\n",
    "@psf.pandas_udf('array<float>')\n",
    "def wikimedia_coordinates_udf(s: pd.Series) -> pd.Series:\n",
    "    def wikimedia_coordinates(wikimedia_text):\n",
    "        try:\n",
    "            p = mwparserfromhell.parse(wikimedia_text)\n",
    "            templates = [t for t in p.filter_templates(recursive=False) if t.name.lower() == 'coord']  \n",
    "            if len(templates) == 0: return None\n",
    "            return interpret_wikimedia_coordinate(templates[0])\n",
    "        except Exception as e:\n",
    "            return None\n",
    "    return(s.apply(wikimedia_coordinates))\n",
    "spark.udf.register('wikimedia_coordinates_udf',wikimedia_coordinates_udf)\n",
    "\n",
    "@psf.pandas_udf('array<string>')\n",
    "def all_h3_grids_udf(lat: pd.Series,lon: pd.Series) -> pd.Series:\n",
    "    result = [ [h3.geo_to_h3(lat, lon, lvl)  for lvl in range(16)] for la,lo in zip(lat,lon)]\n",
    "    print(result)\n",
    "    return pd.Series(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "antique-christian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing Frankie Yankovic in the driver\n",
      "parsing all 7 using the udf\n",
      "+------------------------------+--------------------------------------------------+-------------------------------+\n",
      "|                         title|                                            coords|                            dob|\n",
      "+------------------------------+--------------------------------------------------+-------------------------------+\n",
      "|              Frankie Yankovic|[{41.569, -81.5752, [802bfffffffffff, 812abffff...|                           null|\n",
      "|                        Veneto|[{45.733334, 11.85, [801ffffffffffff, 811ebffff...|                           null|\n",
      "|                      Lombardy|[{45.585556, 9.930278, [801ffffffffffff, 811fbf...|                           null|\n",
      "|             Odense University|[{55.369232, 10.428814, [801ffffffffffff, 811f3...|                           null|\n",
      "|University of Southern Denmark|[{55.36861, 10.428056, [801ffffffffffff, 811f3f...|                           null|\n",
      "|          Charles James Napier|                                                []|{{birth date|1782|8|10|df=yes}}|\n",
      "|          Hans Albert Einstein|                                                []|       {{Birth date|1904|5|14}}|\n",
      "+------------------------------+--------------------------------------------------+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_mediawiki_parser_udf = True\n",
    "if test_mediawiki_parser_udf:\n",
    "    infoboxes_with_locations_df = spark.sql('''\n",
    "        select * \n",
    "          from wikipedia_bronze \n",
    "          where lower(body) like '%{{infobox%' and lower(body) like '%{{coord%' \n",
    "          limit 5\n",
    "    ''').cache()\n",
    "    person_infoboxes_df = spark.sql('''\n",
    "          select * from (select * \n",
    "            from wikipedia_bronze \n",
    "            where title in ('Hans Albert Einstein', 'Charles James Napier')\n",
    "            limit 5) as person_infoboxes\n",
    "    ''').cache()\n",
    "    interesting_test_cases_df = infoboxes_with_locations_df.union(person_infoboxes_df)\n",
    "    interesting_test_cases_df.createOrReplaceTempView('interesting_test_casess')\n",
    "    print(f'parsing {interesting_test_cases_df.take(1)[0].title} in the driver')\n",
    "    txt = interesting_test_cases_df.take(1)[0].body\n",
    "    json.loads(extract_mediawiki_info(txt))\n",
    "    print(f'parsing all {interesting_test_cases_df.count()} using the udf')\n",
    "    df1 = interesting_test_cases_df\n",
    "    df2 = df1.withColumn('parsed_elements',extract_mediawiki_info_udf('body'))\n",
    "    df3 = df2.withColumn('parsed_elements',psf.from_json('parsed_elements',parsed_schema))\n",
    "    df3.selectExpr('title',\n",
    "                   'parsed_elements.coords',\n",
    "                   \"parsed_elements.infoboxes[0].params['birth_date'] as dob\"\n",
    "                  ).show(30,50)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "changing-provision",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+------------------------------------------------------------------------------------------+\n",
      "|                 latlon|                                                                                  testcase|\n",
      "+-----------------------+------------------------------------------------------------------------------------------+\n",
      "|                   null|                                                                                       ...|\n",
      "| [43.651234, -79.38333]|        {{coord|43.651234|-79.383333}}     43.651234°N 79.383333°W    Toronto – Fully d...|\n",
      "|        [43.65, -79.38]|          {{coord|43.65|-79.38}}     43.65°N 79.38°W    Toronto – low precision decimal...|\n",
      "|        [43.65, -79.38]|        {{coord|43.6500|-79.3800}}     43.6500°N 79.3800°W     Toronto – medium precisi...|\n",
      "|     [43.6535, -79.384]|        {{coord|43.653500|N|79.384000|W}}     43.653500°N 79.384000°W     Toronto – hig...|\n",
      "| [43.483334, -79.38333]|          {{coord|43|29|N|79|23|W}}     43°29′N 79°23′W     Toronto – degrees & minutes...|\n",
      "| [43.486946, -79.38389]|        {{coord|43|29|13|N|79|23|02|W}}     43°29′4″N 79°23′0″W     Toronto – degrees, ...|\n",
      "|    [43.48683, -79.384]|        {{coord|43|29|12.6|N|79|23|02.4|W}}     43°29′12.6″N 79°23′02.4″W     Toronto –...|\n",
      "|  [55.75222, 37.615555]|        {{coord|55.752222|N|37.615556|E}}     55.752222°N 37.615556°E     Moscow – N & ...|\n",
      "|  [55.75222, 37.615555]|        {{coord|55.752222|N|37.615556|E|format=dms}}     55°45′08″N 37°36′56″E     Conv...|\n",
      "| [39.098095, -94.58731]|        {{coord|39.098095|-94.587307|format=dms}}     39°05′53″N 94°35′14″W     Decimal...|\n",
      "|  [55.75222, 37.615555]|        {{coord|55.752222|N|37.615556|E|format=dec|name=Moscow}}     55.752222°N 37.615...|\n",
      "|[-33.916668, 18.416666]|                    {{coord|33|55|S|18|25|E}}     33°55′S 18°25′E     Cape Town – S & E...|\n",
      "|          [35.0, 105.0]|         {{coord|35|00|N|105|00|E}}     35°00′N 105°00′E     People's Republic of China...|\n",
      "|[-22.908333, -43.24361]|              {{coord|22|54|30|S|43|14|37|W}}     22°54′30″S 43°14′37″W     Rio – S & W...|\n",
      "|         [-22.0, -43.0]|                             {{coord|22|S|43|W}}     22°S 43°W     A degree confluence....|\n",
      "|[52.466667, -1.9166666]|        {{coord|52|28|N|1|55|W|region:GB_type:city|notes=<ref>{{cite web|url=http://www...|\n",
      "| [51.430218, 0.7324167]|        {{coord|51|25.813|N|0|43.945|E}}     51°25.813′N 0°43.945′E     Navigation buoy...|\n",
      "| [51.604782, -8.533633]|        {{coord|51|36.287|N|8|32.018|W}}     51°36.287′N 8°32.018′W     Lighthouse at t...|\n",
      "|                   null|                                                                                       ...|\n",
      "+-----------------------+------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_coords = True\n",
    "if test_coords:\n",
    "    # https://en.wikipedia.org/wiki/Template:Coord\n",
    "    testcases = '''\n",
    "        {{coord|43.651234|-79.383333}}     43.651234°N 79.383333°W    Toronto – Fully decimal – western hemisphere implied by negation\n",
    "        {{coord|43.65|-79.38}}     43.65°N 79.38°W    Toronto – low precision decimal\n",
    "        {{coord|43.6500|-79.3800}}     43.6500°N 79.3800°W     Toronto – medium precision decimal with trailing zeroes\n",
    "        {{coord|43.653500|N|79.384000|W}}     43.653500°N 79.384000°W     Toronto – high precision decimal with explicit hemisphere notation\n",
    "        {{coord|43|29|N|79|23|W}}     43°29′N 79°23′W     Toronto – degrees & minutes\n",
    "        {{coord|43|29|13|N|79|23|02|W}}     43°29′4″N 79°23′0″W     Toronto – degrees, minutes & seconds\n",
    "        {{coord|43|29|12.6|N|79|23|02.4|W}}     43°29′12.6″N 79°23′02.4″W     Toronto – degrees, minutes, seconds & fractions of seconds\n",
    "        {{coord|55.752222|N|37.615556|E}}     55.752222°N 37.615556°E     Moscow – N & E\n",
    "        {{coord|55.752222|N|37.615556|E|format=dms}}     55°45′08″N 37°36′56″E     Convert to dms format\n",
    "        {{coord|39.098095|-94.587307|format=dms}}     39°05′53″N 94°35′14″W     Decimal conversion without N/S/E/W\n",
    "        {{coord|55.752222|N|37.615556|E|format=dec|name=Moscow}}     55.752222°N 37.615556°E     Convert to decimal and label on some maps\n",
    "        {{coord|33|55|S|18|25|E}}     33°55′S 18°25′E     Cape Town – S & E\n",
    "        {{coord|35|00|N|105|00|E}}     35°00′N 105°00′E     People's Republic of China\n",
    "        {{coord|22|54|30|S|43|14|37|W}}     22°54′30″S 43°14′37″W     Rio – S & W\n",
    "        {{coord|22|S|43|W}}     22°S 43°W     A degree confluence.\n",
    "        {{coord|52|28|N|1|55|W|region:GB_type:city|notes=<ref>{{cite web|url=http://www.fallingrain.com/world/UK/0/Birmingham.html|title=Birmingham}}</ref>|display=inline,title}}     52°28′N 1°55′W[1]Coordinates: 52°28′N 1°55′W[1]     Birmingham – with display, notes, and parameter settings; note that these coordinates are also displayed at the top of this page.\n",
    "        {{coord|51|25.813|N|0|43.945|E}}     51°25.813′N 0°43.945′E     Navigation buoy in the River Medway, England.\n",
    "        {{coord|51|36.287|N|8|32.018|W}}     51°36.287′N 8°32.018′W     Lighthouse at the Old Head of Kinsale as defined by the Commissioners of Irish Lights. \n",
    "    '''.splitlines()\n",
    "    spark.createDataFrame([[testcase] for testcase in testcases],'testcase string').createOrReplaceTempView('coord_testcases')\n",
    "    spark.sql('''select wikimedia_coordinates_udf(testcase) as latlon,substr(testcase,1,90)||'...' as testcase from coord_testcases''').show(40,90)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aboriginal-trauma",
   "metadata": {},
   "source": [
    "#### Apply the mwparserfromhell parser on each document.\n",
    "\n",
    "* Work around memory-related limitations on tiny/cheap clusters (Azure's cheapest tier of Databricks workers, or a local desktop).\n",
    "\n",
    "The default`spark.sql.execution.arrow.maxRecordsPerBatch` of 10,000 doesn't work well when you hit ranges of some of the larger documents like\n",
    "\n",
    "    {\n",
    "       \"title\": \"Wikipedia:Arbitration Committee Elections December 2018/Coordination/Mass message\",\n",
    "       \"length(body)\": 4483016\n",
    "    },\n",
    "    {\n",
    "       \"title\": \"Wikipedia:CHECKWIKI/WPC 055 dump\",\n",
    "       \"length(body)\": 2472791\n",
    "    },\n",
    "\n",
    "and https://en.wikipedia.org/wiki/Help:Citation_Style_1/mass_test/fcite_web_ref\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "grave-central",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "# even with 360 partitions, I get OOM errors\n",
    "# defaults to 10,000 which is painful on large wikipedia articles\n",
    "#\n",
    "# Especially ones like\n",
    "\n",
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\",1000)  \n",
    "#spark.conf.set(\"\",5)\n",
    "# 3% MEM at 125/341\n",
    "\n",
    "df1 = spark.sql(\"select * from wikipedia_bronze\")\n",
    "\n",
    "# Tricky workaround for memory leak in the UDFs defined above.\n",
    "#\n",
    "# Run against all documents in Wikipedia, it will leak (not use.. unnecessarily grow by)\n",
    "# about 40GB.  If spark splits this among 10 executors, each will grow by about 4GB, which\n",
    "# still exceeds the 32GB cheap cluster I'm using.   By splitting the dataframe into 10 batches,\n",
    "# it kills the \"python3 -m pyspark.daemon\" workers after each 10th of the data (so never\n",
    "# leaking over 4GB).  Using .repartition() won't have the same benefits because it seems \n",
    "# the task re-uses the \"python3 -m pyspark.daemon\" for multiple partitions.\n",
    "num_batches = 10\n",
    "df2s = [(df1.filter(f'((xxhash64(title)&9223372036854775807) % {num_batches}) = {i}')\n",
    "            .withColumn('parsed_elements',extract_mediawiki_info_udf('body')))\n",
    "        for i in range(num_batches)\n",
    "       ]\n",
    "df2 = functools.reduce(lambda x,y: x.union(y),df2s)\n",
    "df3 = df2.withColumn('parsed_elements',psf.from_json('parsed_elements',parsed_schema))\n",
    "df4 = df3.select('title','body','parsed_elements.*')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "diagnostic-selection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will take about 3 hours on an 8-core (16-vcore) cluster; about 1 hour on a 48-vcore cluster.\n",
    "reparse_wikimedia_markup = False\n",
    "if reparse_wikimedia_markup:\n",
    "    spark.sql(\"drop table if exists wikipedia_silver_structured_templates\")\n",
    "    df4.write.format('delta').saveAsTable('wikipedia_silver_structured_templates')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instrumental-buddy",
   "metadata": {},
   "source": [
    "## Spot check structured wikipedia data\n",
    "\n",
    "At this point we can query the infoboxes, coordinates, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "refined-wireless",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(count(1)=21108360, count(DISTINCT title)=21108326)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('select count(*),count(distinct title) from  wikipedia_silver_structured_templates').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "comprehensive-lloyd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------+----------+\n",
      "|                         wikipedia_pages_with_multiple_coordinates|num_coords|\n",
      "+------------------------------------------------------------------+----------+\n",
      "|Wikipedia:WikiProject National Register of Historic Places/coordsH|      3692|\n",
      "|                     Sortable list of islands of Western Australia|      1139|\n",
      "|                                          List of schools in Perak|      1103|\n",
      "|                                      List of lakes of New Zealand|      1082|\n",
      "|                                     List of places in Colorado-02|      1077|\n",
      "|                                        List of places in Colorado|      1031|\n",
      "|                  List of cities, towns and villages in Gelderland|       842|\n",
      "|                                 List of waterfalls of Nova Scotia|       832|\n",
      "|       List of cities, towns and villages in Limburg (Netherlands)|       770|\n",
      "|                                      List of villages in Vestland|       765|\n",
      "+------------------------------------------------------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "  select title as wikipedia_pages_with_multiple_coordinates,\n",
    "         size(coords) as num_coords \n",
    "  from wikipedia_silver_structured_templates \n",
    "  order by size(coords) desc \n",
    "  limit 10\n",
    "''').show(100,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "surrounded-retro",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75739 parsing errors\n",
      "+---------------------------------------------------------+------------------------------------------------+\n",
      "|                                                    title|                                         _error_|\n",
      "+---------------------------------------------------------+------------------------------------------------+\n",
      "|        2021 FIVB Volleyball Boys' U19 World Championship|Object of type Wikicode is not JSON serializable|\n",
      "|                                           Robert Kallman|        could not convert string to float: 'LAT'|\n",
      "|                             Totonicapán Uprising of 1820|                                             '1'|\n",
      "|Wikipedia:Administrators' noticeboard/IncidentArchive1049|Object of type Wikicode is not JSON serializable|\n",
      "|                         HPV Prevention and Control Board|        could not convert string to float: 'LAT'|\n",
      "|  Henry J. Carter Specialty Hospital and Nursing Facility|                                             '1'|\n",
      "|                                          Draft:Ruby Brar|        could not convert string to float: 'LAT'|\n",
      "|                      Draft:PhilHealth corruption scandal|        could not convert string to float: 'LAT'|\n",
      "|                                         Lansana Mansaray|        could not convert string to float: 'LAT'|\n",
      "|                                         Draft:Pratik Sen|        could not convert string to float: 'LAT'|\n",
      "|                                        Draft:Naveed Khan|        could not convert string to float: 'LAT'|\n",
      "|                              The Five Senses (Stoskopff)|        could not convert string to float: 'LAT'|\n",
      "|        Template:George Floyd protests map/non-US/sandbox|       could not convert string to float: '±LAT'|\n",
      "|                                         Vivienne Roumani|        could not convert string to float: 'LAT'|\n",
      "|                                          Vahagn Asatryan|        could not convert string to float: 'LAT'|\n",
      "|                                   Draft:Shammah Chenhaka|Object of type Wikicode is not JSON serializable|\n",
      "|                             Draft:Pavol Bohdan Zápotočný|        could not convert string to float: 'LAT'|\n",
      "|                                          Stapelen Castle|                                             '1'|\n",
      "|                                   Sikiru Ajibowu Adeyiga|        could not convert string to float: 'LAT'|\n",
      "|                                       Draft:Funny Mmasco|        could not convert string to float: 'LAT'|\n",
      "+---------------------------------------------------------+------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql('select title,_error_ from wikipedia_silver_structured_templates where _error_ is not null')\n",
    "print(f'{df.count()} parsing errors')\n",
    "# TODO - fix these later\n",
    "# for row in df.take(100): print(json.dumps(row.asDict(True)))\n",
    "df.show(20,80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "engaged-intent",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_large_articles = False\n",
    "if debug_large_articles:\n",
    "    sizes = spark.sql('''\n",
    "     select title,length(body) \n",
    "       from wikipedia_silver_structured_templates \n",
    "      where title not like 'Wikipedia:%' \n",
    "        and title not like 'Template:%' \n",
    "        and title not like 'Module:%' \n",
    "        and title not like 'MediaWiki:%'\n",
    "        and title not like 'Template:%' \n",
    "        and title not like 'Draft:%' \n",
    "        and title not like 'Help:%' \n",
    "        order by length(body) desc limit 10\n",
    "     ''').cache()\n",
    "    sizes.show(40,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "stopped-discovery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|articles_with_coords|num_coords|\n",
      "+--------------------+----------+\n",
      "|             1203242|   1855143|\n",
      "+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''select count(*) as articles_with_coords,sum(size(coords)) as num_coords\n",
    "    from wikipedia_silver_structured_templates \n",
    "    where size(coords) > 0\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "necessary-punishment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_h3</th>\n",
       "      <th>num_h4</th>\n",
       "      <th>num_h5</th>\n",
       "      <th>num_h6</th>\n",
       "      <th>num_h7</th>\n",
       "      <th>num_h8</th>\n",
       "      <th>num_h9</th>\n",
       "      <th>count(1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21799</td>\n",
       "      <td>72608</td>\n",
       "      <td>206136</td>\n",
       "      <td>498461</td>\n",
       "      <td>883727</td>\n",
       "      <td>1164699</td>\n",
       "      <td>1335738</td>\n",
       "      <td>1855143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_h3  num_h4  num_h5  num_h6  num_h7   num_h8   num_h9  count(1)\n",
       "0   21799   72608  206136  498461  883727  1164699  1335738   1855143"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# About a half million h6 grids with information in wikipedia.\n",
    "spark.sql('''\n",
    "  select count(distinct coord.h3[3]) as num_h3,\n",
    "         count(distinct coord.h3[4]) as num_h4,\n",
    "         count(distinct coord.h3[5]) as num_h5,\n",
    "         count(distinct coord.h3[6]) as num_h6,\n",
    "         count(distinct coord.h3[7]) as num_h7,\n",
    "         count(distinct coord.h3[8]) as num_h8,\n",
    "         count(distinct coord.h3[9]) as num_h9,\n",
    "         count(*)\n",
    " from wikipedia_silver_structured_templates\n",
    " lateral view explode(coords) as coord\n",
    "''').toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agreed-handbook",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "* http://mcburton.net/blog/static-files/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "standing-turkey",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+---------------------------------------------+\n",
      "|title                        |body                                         |\n",
      "+-----------------------------+---------------------------------------------+\n",
      "|Black panther                |{{short description|Melanistic colour va...  |\n",
      "|Great white shark            |{{other uses of|great white|Great White ...  |\n",
      "|Melisende, Queen of Jerusalem|{{short description|Queen regnant of the...  |\n",
      "|Carcharodon carcharias       |#REDIRECT [[Great white shark]] \\n \\n {{Re...|\n",
      "|Method                       |{{Wiktionary|method}} \\n '''Method''' ({{... |\n",
      "+-----------------------------+---------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(r\"select title,replace(substr(body,1,40)||'...','\\n','\\\\n') as body from wikipedia_bronze limit 5\").show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "wireless-ethics",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='demo.html' target='_blank'>demo.html</a><br>"
      ],
      "text/plain": [
       "/home/jovyan/work/wikipedia_in_spark/notebooks/demo.html"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import FileLink, FileLinks\n",
    "FileLink('demo.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
