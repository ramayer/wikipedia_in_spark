{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "impressed-dairy",
   "metadata": {},
   "source": [
    "# Preprocess Wikipedia with Spark\n",
    "\n",
    "## TL/DR:\n",
    "\n",
    "1. Start with a Wikipedia Dump\n",
    "2. Process the data in Spark.\n",
    "3. Produce maps like this: ![alt text](../assets/all_wikipedia_coords.png \"Title\")\n",
    "\n",
    "Resources:\n",
    "* https://en.wikipedia.org/wiki/Wikipedia:Database_download\n",
    "* https://towardsdatascience.com/wikipedia-data-science-working-with-the-worlds-largest-encyclopedia-c08efbac5f5c\n",
    "* https://github.com/earwig/mwparserfromhell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afraid-contract",
   "metadata": {},
   "source": [
    "## Install dependencies\n",
    "\n",
    "This is using `%pip` rather than `pkg_rsources.resolve()` because on databricks clusters, `%pip` will make sure the libraries are available on the spark worker nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "temporal-wallpaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "required_packages = {\"mwparserfromhell\",\"geopandas\",\"h3\",\"geocoder\",\"pydeck\"}\n",
    "import pkg_resources\n",
    "for lib in required_packages - {pkg.key for pkg in pkg_resources.working_set}:\n",
    "    print(f\"installing {lib}\")\n",
    "    %pip install -q --upgrade pip\n",
    "    %pip install -q $lib\n",
    "    pkg_resources.require(lib)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "third-impossible",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to find (and if needed download) a wikipedia dump file\n",
    "source_dir      = './data/wikipedia_src'\n",
    "source_dump     = source_dir + '/enwiki-latest-pages-articles-multistream.xml.bz2'\n",
    "\n",
    "# Where to store temporary pre-processed files before creating spark tables.  Will temporarily use ~70GB.\n",
    "preprocess_dir  = './data/wikipedia_as_json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "guided-emission",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.sax\n",
    "import json\n",
    "import mwparserfromhell\n",
    "import subprocess\n",
    "import json\n",
    "import time\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artificial-composition",
   "metadata": {},
   "source": [
    "## Fetch the latest Wikipedia dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sensitive-server",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘enwiki-latest-pages-articles-multistream.xml.bz2’ already there; not retrieving.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! mkdir -p data/wikipedia_src data/wikipedia_as_json\n",
    "! (cd data/wikipedia_src; wget -nc https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles-multistream.xml.bz2)\n",
    "# !bunzip2 -c <  './data/wikipedia_src/enwiki-latest-pages-articles-multistream.xml.bz2' | head -100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "productive-favorite",
   "metadata": {},
   "source": [
    "## Pre-process the Wikimedia XML to JSONL / NLJSON\n",
    "\n",
    "* Spark's tooling for JSONL is better than it's XML handling so pre-process the XML to JSONL.\n",
    "\n",
    "* Put 100,000 Wikipeda pages in each .json file.\n",
    "    * JSON files each containing 10,000 to 100,000 Wikipedia pages seems to be a sweet spot for loading as Spark dataframes on this cluster.\n",
    "    * This generates about 215 files, averaging 325MB each.\n",
    "\n",
    "* Yes, this preprocessing step could be further optimized, but it doesn't matter.\n",
    "    * The XML->JSON conversion is fast enough it doesn't need to be parallized. It takes under 2 hours on a single CPU core.  In contrast, parsing the wikimedia markup below takes many hours on a cluster of 32 CPUs.  Compressing the temp files could save space, but it doesn't matter either.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "southeast-integrity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based heavily on\n",
    "# https://github.com/WillKoehrsen/wikipedia-data-science/blob/master/notebooks/Downloading%20and%20Parsing%20Wikipedia%20Articles.ipynb\n",
    "\n",
    "class WikiXmlHandler(xml.sax.handler.ContentHandler):\n",
    "    \"\"\"Content handler for Wiki XML data using SAX\"\"\"\n",
    "    def __init__(self):\n",
    "        xml.sax.handler.ContentHandler.__init__(self)\n",
    "        self._buffer = None\n",
    "        self._values = {}\n",
    "        self._current_tag = None\n",
    "        self._pages = []\n",
    "        self._page_count = 0\n",
    "        self._in_revision = False\n",
    "        self._interesting_elements = ('title', 'text', 'timestamp', 'model', 'format')\n",
    "\n",
    "    def characters(self, content):\n",
    "        \"\"\"Characters between opening and closing tags\"\"\"\n",
    "        if self._current_tag:\n",
    "            self._buffer.append(content)\n",
    "\n",
    "    def startElement(self, name, attrs):\n",
    "        \"\"\"Opening tag of element\"\"\"\n",
    "        if name == 'revision': self._in_revision = True\n",
    "        self._current_tag = name\n",
    "        self._buffer = []\n",
    "\n",
    "    def endElement(self, name):\n",
    "        \"\"\"Closing tag of element\"\"\"\n",
    "        if name == 'revision': self._in_revision = False\n",
    "        if (name in self._interesting_elements) or (name == 'id' and not self._in_revision):\n",
    "            self._values[name] = ' '.join(self._buffer)\n",
    "        if name == 'page':\n",
    "            self._pages.append(self._values)\n",
    "            self._values = {}\n",
    "            \n",
    "    def save_partial_results(self,path):\n",
    "        self._page_count += len(self._pages)\n",
    "        filename = f'{path}/{self._page_count}.jsonl'\n",
    "        with open(filename,'w') as jsonfile:\n",
    "            for page in self._pages:\n",
    "                page_dict = page\n",
    "                json.dump(page_dict,jsonfile)\n",
    "                jsonfile.write(\"\\n\")\n",
    "        self._pages = []\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "annoying-departure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes roughly 2 hours\n",
    "rebuild_json_files = False\n",
    "if rebuild_json_files:\n",
    "    wikipedia_dump = source_dump\n",
    "    handler        = WikiXmlHandler()\n",
    "    parser         = xml.sax.make_parser()\n",
    "    parser.setContentHandler(handler)\n",
    "    expected_pages = 21100000  # Doesn't need to be exact. Just used to show progress.\n",
    "    t0 = time.time()\n",
    "    for i, line in enumerate(subprocess.Popen(['bzcat'], \n",
    "                             stdin = open(wikipedia_dump), \n",
    "                             stdout = subprocess.PIPE).stdout):\n",
    "        parser.feed(line)\n",
    "        if len(handler._pages) >= 100000:\n",
    "            handler.save_partial_results(preprocess_dir)\n",
    "            t1 = time.time()\n",
    "            et = t1 - t0\n",
    "            page_count = handler._page_count\n",
    "            print(f'page {page_count} at {int(et)} seconds '\n",
    "                  f'({int(page_count/(et))} per second). '\n",
    "                  f'Estimate {int(et*expected_pages/page_count/60)} minutes '\n",
    "                  f'{et*(expected_pages-page_count)/page_count/60} remaining'\n",
    "                 )\n",
    "    handler.save_partial_results(preprocess_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thick-equivalent",
   "metadata": {},
   "source": [
    "## Launch Spark (if running on a standalone environment)\n",
    "\n",
    "* On databricks clusters the Spark Context will already have existed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "established-presence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://08651cc65d10:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>MyApp</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f4966608370>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not \"spark\" in locals():\n",
    "    import pyspark\n",
    "    MAX_MEMORY = \"8g\"  # 24 gives OOM here. # 6 gives \"out of heap space\"\n",
    "    spark = (pyspark.sql.SparkSession.builder.appName(\"MyApp\") \n",
    "        .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.0.0\") \n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \n",
    "        .config(\"spark.executor.memory\", MAX_MEMORY) \n",
    "        .config(\"spark.driver.memory\", MAX_MEMORY) \n",
    "        .config(\"spark.python.worker.reuse\",False)\n",
    "        .config(\"spark.task.maxFailures\",5)\n",
    "        .enableHiveSupport() \n",
    "        .getOrCreate()        \n",
    "        )\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "distinguished-version",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved as delta table in 243.2141054358799 seconds\n"
     ]
    }
   ],
   "source": [
    "rebuild_delta_table = False\n",
    "if rebuild_delta_table:\n",
    "    t0 = time.perf_counter()\n",
    "    df = spark.read.json(preprocess_dir,'title string, text string, id string, timestamp string, model string, format string')\n",
    "    spark.sql(\"drop table if exists wikipedia_bronze\")\n",
    "    df.select('title','text','id','timestamp', 'model','format').write.format('delta').saveAsTable('wikipedia_bronze')\n",
    "    print(f\"saved as delta table in {time.perf_counter() - t0} seconds\")\n",
    "    #napier_df = spark.sql(\"select * from wikipedia_bronze where title = 'Charles James Napier' limit 3\").cache()\n",
    "    #napier_df.write.format('delta').saveAsTable('wikipedia_bronze_small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "outside-orange",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+----------------+\n",
      "|count(1)|               model|          format|\n",
      "+--------+--------------------+----------------+\n",
      "|21099396|            wikitext|     text/x-wiki|\n",
      "|    7984|           Scribunto|      text/plain|\n",
      "|     442|       sanitized-css|        text/css|\n",
      "|     313|          javascript| text/javascript|\n",
      "|     121|                 css|        text/css|\n",
      "|      74|MassMessageListCo...|application/json|\n",
      "|      29|                json|application/json|\n",
      "|       1|                text|      text/plain|\n",
      "+--------+--------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select count(*),model,format from wikipedia_bronze group by model,format order by count(*) desc\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "interstate-raleigh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+---------------------------------------------------------------------------------------+\n",
      "|title                        |body                                                                                   |\n",
      "+-----------------------------+---------------------------------------------------------------------------------------+\n",
      "|Karel Čapek                  |{{Short description|Early 20th-century Czech writer and playwright famous for hi...    |\n",
      "|Minguo                       |#REDIRECT [[Republic of China (disambiguation)]]...                                    |\n",
      "|Relativity theory            |#Redirect [[Theory of relativity]] \\n \\n \\n {{Redirect category shell|1= \\n {{Redire...|\n",
      "|Nine men's morris            |{{Short description|strategy board game}} \\n {{Infobox Game \\n  |title =  \\n  |ital... |\n",
      "|Wars of Scottish Independence|{{Short description|War of national liberation between Scotland and England}} \\n ...   |\n",
      "+-----------------------------+---------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(r\"select title,replace(substr(text,1,80)||'...','\\n','\\\\n') as body from wikipedia_bronze limit 5\").show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desperate-arlington",
   "metadata": {},
   "source": [
    "## Parse the wikipedia content\n",
    "\n",
    "* Wraps the excellent but pretty slow `mwparserfromhell` library: https://github.com/earwig/mwparserfromhell\n",
    "\n",
    "* Attempts to have this Pandas UDF return comples structures like \n",
    "\n",
    "      infoboxes array<struct<name:string,params:map<string,string>,body:string>>,\n",
    "      templates array<struct<name:string,params:map<string,string>,body:string>>,\n",
    "      extlinks  array<struct<title:string,url:string>>,\n",
    "      wikilinks array<struct<text:string,title:string>>,\n",
    "      _error_   string\n",
    "      \n",
    "    are failing with the following in Spark 3.1.1:\n",
    "\n",
    "        21/04/15 07:05:29 WARN BlockManager: Putting block rdd_66_0 failed due to exception org.apache.spark.TaskKilledException.\n",
    "        21/04/15 07:05:29 ERROR TaskContextImpl: Error in TaskCompletionListener\n",
    "        java.lang.IllegalStateException: Memory was leaked by query. Memory leaked: (4194304)\n",
    "        Allocator(stdin reader for python3) 0/4194304/12602208/9223372036854775807 (res/actual/peak/limit)\n",
    "        at org.apache.arrow.memory.BaseAllocator.close(BaseAllocator.java:431)\n",
    "    \n",
    "* Work around it by returning the data encoded as a json string.    \n",
    "  Doesn't add much overhead compared to the actual parsing.\n",
    "  \n",
    "* Consider\n",
    " * https://www.dbpedia.org/ - which already has parsed infoboxes\n",
    " * https://pypi.org/project/mwlib/ - a parser that can make printable pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "amazing-thickness",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as psf\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import h3 as h3\n",
    "\n",
    "def all_h3_grids(lat:float,lon:float) -> list:   # TODO list[str] in 3.9\n",
    "    \"\"\"Return an array of all h3 grids for a lat,lon pair.\"\"\"\n",
    "    return([h3.geo_to_h3(lat, lon, lvl)  for lvl in range(16)])\n",
    "\n",
    "\n",
    "def interpret_wikimedia_coordinate(parsed_template:mwparserfromhell.nodes.template.Template):\n",
    "    \"\"\"Parse the many wikimedia coord template formats.\n",
    "\n",
    "    Note that wikimedia has many formats for {{coord...}} templates.\n",
    "\n",
    "    Args:\n",
    "        parsed_template: a parsed template\n",
    "\n",
    "    Returns:\n",
    "        list[float]: [latitude,longitude]\n",
    "         \n",
    "    Examples:\n",
    "        https://en.wikipedia.org/wiki/Template:Coord\n",
    "        {{coord|latitude|longitude|coordinate parameters|template parameters}}\n",
    "        {{coord|dd|N/S|dd|E/W|coordinate parameters|template parameters}}\n",
    "        {{coord|dd|mm|N/S|dd|mm|E/W|coordinate parameters|template parameters}}\n",
    "        {{coord|dd|mm|ss|N/S|dd|mm|ss|E/W|coordinate parameters|template parameters}}\n",
    "\n",
    "    \"\"\"\n",
    "    params = {p.name.strip(): p.value.strip() for p in parsed_template.params}\n",
    "    lat = lon = None\n",
    "    if params.get('1') == 'LAT': return None   # a common unfinished template\n",
    "    if not (params.get('4') in ['N','S']\n",
    "            or \n",
    "            params.get('3') in ['N','S']\n",
    "            or \n",
    "            params.get('2') in ['N','S']\n",
    "           ):\n",
    "        lat = float(params['1'])\n",
    "        lon = float(params['2'])\n",
    "    if params.get('4') == 'N': lat =    int(params['1']) + float(params['2'])/60 + float(params['3'])/60/60\n",
    "    if params.get('4') == 'S': lat = - (int(params['1']) + float(params['2'])/60 + float(params['3'])/60/60)\n",
    "    if params.get('8') == 'E': lon =    int(params['5']) + float(params['6'])/60 + float(params['7'])/60/60\n",
    "    if params.get('8') == 'W': lon = - (int(params['5']) + float(params['6'])/60 + float(params['7'])/60/60)\n",
    "    if params.get('3') == 'N': lat =    int(params['1']) + float(params['2'])/60 \n",
    "    if params.get('3') == 'S': lat = - (int(params['1']) + float(params['2'])/60 )\n",
    "    if params.get('6') == 'E': lon =    int(params['4']) + float(params['5'])/60 \n",
    "    if params.get('6') == 'W': lon = - (int(params['4']) + float(params['5'])/60 )\n",
    "    if params.get('2') == 'N': lat =    float(params['1'])\n",
    "    if params.get('2') == 'S': lat = - (float(params['1']) )\n",
    "    if params.get('4') == 'E': lon =    float(params['3'])\n",
    "    if params.get('4') == 'W': lon = - (float(params['3']) )\n",
    "    if lat and lon: return [lat,lon]\n",
    "    return None\n",
    "\n",
    "def interpret_wikimedia_dob(parsed_template):\n",
    "    \"\"\"Parse the many wikimedia birthdate template formats.\n",
    "\n",
    "    \"\"\"\n",
    "    params = {p.name.strip(): p.value.strip() for p in parsed_template.params}\n",
    "    print(params)\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "# https://en.wikipedia.org/wiki/Template:Birth_date\n",
    "def extract_mediawiki_info(wikimedia_text):\n",
    "    def nstr(s): \n",
    "        return(s and s.strip())\n",
    "    def params_as_dict(tmpl):\n",
    "        return({p.name.strip(): p.value.strip() for p in tmpl.params})\n",
    "    def tmpl_as_dict(tmpl):\n",
    "        return({\"name\":nstr(tmpl.name),\"params\":params_as_dict(tmpl),\"body\":nstr(tmpl)})\n",
    "    try:\n",
    "        p = mwparserfromhell.parse(wikimedia_text)\n",
    "        all_tmpl    = p.filter_templates(recursive=True) \n",
    "        top_tmpl    = p.filter_templates(recursive=False)\n",
    "        raw_coords  = [t for t in all_tmpl if t.name.strip().lower() == 'coord']\n",
    "        lat_lon = [interpret_wikimedia_coordinate(coord) for coord in raw_coords if coord]\n",
    "        coords  = [{'lat':coord[0],\n",
    "                    'lon':coord[1],\n",
    "                    'h3':all_h3_grids(coord[0],coord[1])\n",
    "                  } for coord in lat_lon]\n",
    "        templates   = [tmpl_as_dict(t) for t in top_tmpl]\n",
    "        infoboxes   = [t for t in templates if     t['name'].lower().startswith('infobox') ]\n",
    "        othertmpl   = [t for t in templates if not t['name'].lower().startswith('infobox') ]\n",
    "        wikilinks   = [{\"title\":nstr(l.title),\"text\":nstr(l.text)} for l in p.filter_wikilinks()]\n",
    "        extlinks    = [{\"title\":nstr(l.title),\"url\":nstr(l.url)}   for l in p.filter_external_links()]\n",
    "        text_only   = p.strip_code()\n",
    "        # TODO - handle dates in person infoboxes here.\n",
    "        return json.dumps({\n",
    "            \"infoboxes\":infoboxes,\n",
    "            \"coords\":coords,\n",
    "            \"templates\":othertmpl,\n",
    "            \"wikilinks\":wikilinks,\n",
    "            \"text_only\":text_only,\n",
    "            \"extlinks\":extlinks\n",
    "        })\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"_error_\":str(e)})\n",
    "\n",
    "parsed_schema = '''\n",
    "      infoboxes array<struct<name:string,params:map<string,string>,body:string>>,\n",
    "      templates array<struct<name:string,params:map<string,string>,body:string>>,\n",
    "      extlinks  array<struct<title:string,url:string>>,\n",
    "      wikilinks array<struct<text:string,title:string>>,\n",
    "      coords    array<struct<lat:float,lon:float,h3:array<string>>>,\n",
    "      text_only string,\n",
    "      _error_   string\n",
    "    '''\n",
    "@psf.pandas_udf('string')\n",
    "def extract_mediawiki_info_udf(s: pd.Series) -> pd.Series:\n",
    "    return s.apply(extract_mediawiki_info)\n",
    "spark.udf.register('extract_mediawiki_info_udf',extract_mediawiki_info_udf)\n",
    "\n",
    "def template_params(wikimedia_text):\n",
    "    try:\n",
    "        p = mwparserfromhell.parse(wikimedia_text)\n",
    "        templates   = p.filter_templates(recursive=False)\n",
    "        if len(templates) == 0: return None\n",
    "        params = {p.name.strip(): p.value.strip() for p in templates[0].params}\n",
    "        return params\n",
    "    except Exception as e:\n",
    "        return {}\n",
    "\n",
    "@psf.pandas_udf('map<string,string>')\n",
    "def template_params_udf(s: pd.Series) -> pd.Series:\n",
    "    return s.apply(template_params)\n",
    "spark.udf.register('template_params_udf',template_params_udf)\n",
    "\n",
    "@psf.pandas_udf('array<float>')\n",
    "def wikimedia_coordinates_udf(s: pd.Series) -> pd.Series:\n",
    "    def wikimedia_coordinates(wikimedia_text):\n",
    "        try:\n",
    "            p = mwparserfromhell.parse(wikimedia_text)\n",
    "            templates = [t for t in p.filter_templates(recursive=False) if t.name.lower() == 'coord']  \n",
    "            if len(templates) == 0: return None\n",
    "            return interpret_wikimedia_coordinate(templates[0])\n",
    "        except Exception as e:\n",
    "            return None\n",
    "    return(s.apply(wikimedia_coordinates))\n",
    "spark.udf.register('wikimedia_coordinates_udf',wikimedia_coordinates_udf)\n",
    "\n",
    "@psf.pandas_udf('array<string>')\n",
    "def all_h3_grids_udf(lat: pd.Series,lon: pd.Series) -> pd.Series:\n",
    "    result = [ [h3.geo_to_h3(lat, lon, lvl)  for lvl in range(16)] for la,lo in zip(lat,lon)]\n",
    "    print(result)\n",
    "    return pd.Series(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "missing-story",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing Montgomery County, Maryland in the driver\n",
      "parsing all 7 using the udf\n",
      "+---------------------------+--------------------------------------------------+-------------------------------+\n",
      "|                      title|                                            coords|                            dob|\n",
      "+---------------------------+--------------------------------------------------+-------------------------------+\n",
      "|Montgomery County, Maryland|[{39.13638, -77.20424, [802bfffffffffff, 812abf...|                           null|\n",
      "|        Rockville, Maryland|                                              null|                           null|\n",
      "|         Bethesda, Maryland|[{38.984722, -77.11305, [802bfffffffffff, 812ab...|                           null|\n",
      "|    Silver Spring, Maryland|[{39.00244, -77.02079, [802bfffffffffff, 812abf...|                           null|\n",
      "|               Moulin Rouge|[{48.884167, 2.3325, [8019fffffffffff, 811fbfff...|                           null|\n",
      "|       Charles James Napier|                                                []|{{birth date|1782|8|10|df=yes}}|\n",
      "|       Hans Albert Einstein|                                                []|       {{Birth date|1904|5|14}}|\n",
      "+---------------------------+--------------------------------------------------+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_mediawiki_parser_udf = True\n",
    "if test_mediawiki_parser_udf:\n",
    "    infoboxes_with_locations_df = spark.sql('''\n",
    "        select * \n",
    "          from wikipedia_bronze \n",
    "          where lower(text) like '%{{infobox%' and lower(text) like '%{{coord%' \n",
    "          limit 5\n",
    "    ''').cache()\n",
    "    person_infoboxes_df = spark.sql('''\n",
    "          select * from (select * \n",
    "            from wikipedia_bronze \n",
    "            where title in ('Hans Albert Einstein', 'Charles James Napier')\n",
    "            limit 5) as person_infoboxes\n",
    "    ''').cache()\n",
    "    interesting_test_cases_df = infoboxes_with_locations_df.union(person_infoboxes_df)\n",
    "    interesting_test_cases_df.createOrReplaceTempView('interesting_test_casess')\n",
    "    print(f'parsing {interesting_test_cases_df.take(1)[0].title} in the driver')\n",
    "    txt = interesting_test_cases_df.take(1)[0].text\n",
    "    json.loads(extract_mediawiki_info(txt))\n",
    "    print(f'parsing all {interesting_test_cases_df.count()} using the udf')\n",
    "    df1 = interesting_test_cases_df\n",
    "    df2 = df1.withColumn('parsed_elements',extract_mediawiki_info_udf('text'))\n",
    "    df3 = df2.withColumn('parsed_elements',psf.from_json('parsed_elements',parsed_schema))\n",
    "    df3.selectExpr('title',\n",
    "                   'parsed_elements.coords',\n",
    "                   \"parsed_elements.infoboxes[0].params['birth_date'] as dob\"\n",
    "                  ).show(30,50)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "active-hollywood",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Charles James Napier\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1782-08-10'"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row = person_infoboxes_df.take(1)[0]\n",
    "txt = row.text\n",
    "parsed_data = json.loads(extract_mediawiki_info(txt))\n",
    "print(row.title)\n",
    "#print(json.dumps(parsed_data['infoboxes'],indent=1))\n",
    "p = mwparserfromhell.parse(txt)\n",
    "#help(p)\n",
    "#print(p.strip_code())\n",
    "top_tmpl    = p.filter_templates(recursive=False)\n",
    "infoboxes = [t for t in top_tmpl if t.name.lower().startswith('infobox')]\n",
    "dir(infoboxes[0].get('birth_date'))\n",
    "dob_value = infoboxes[0].get('birth_date').value\n",
    "\n",
    "def interpret_wikimedia_dob(dob_value:mwparserfromhell.wikicode.Wikicode) -> str:\n",
    "    \"\"\"Parse the many wikimedia birthdate template formats.\n",
    "    \"\"\"\n",
    "    tmpls = dob_value.filter_templates\n",
    "    if len(tmpls) != 1:\n",
    "        return str(dob_value)\n",
    "    tmpl = tmpls[0]\n",
    "    if lower(tmpl.name) == 'birth date':\n",
    "        y,m,d,*rest =  tmpls[0].params\n",
    "        return(f'{int(str(y)):04}-{int(str(m)):02d}-{int(str(d)):02d}')\n",
    "    print(params)\n",
    "    return None\n",
    "\n",
    "tmpls = dob_value.filter_templates()\n",
    "tmpls[0].name\n",
    "y,m,d,*rest =  tmpls[0].params\n",
    "y,m,d\n",
    "#f'{if'{int(str(y)):04}-{int(str(m)):02d}-{int(str(d)):02d}')nt(y):04}-{int(m):02}-{int(d):02}'\n",
    "f'{int(str(y)):04}-{int(str(m)):02d}-{int(str(d)):02d}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "designing-medline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+------------------------------------------------------------------------------------------+\n",
      "|                 latlon|                                                                                  testcase|\n",
      "+-----------------------+------------------------------------------------------------------------------------------+\n",
      "|                   null|                                                                                       ...|\n",
      "| [43.651234, -79.38333]|        {{coord|43.651234|-79.383333}}     43.651234°N 79.383333°W    Toronto – Fully d...|\n",
      "|        [43.65, -79.38]|          {{coord|43.65|-79.38}}     43.65°N 79.38°W    Toronto – low precision decimal...|\n",
      "|        [43.65, -79.38]|        {{coord|43.6500|-79.3800}}     43.6500°N 79.3800°W     Toronto – medium precisi...|\n",
      "|     [43.6535, -79.384]|        {{coord|43.653500|N|79.384000|W}}     43.653500°N 79.384000°W     Toronto – hig...|\n",
      "| [43.483334, -79.38333]|          {{coord|43|29|N|79|23|W}}     43°29′N 79°23′W     Toronto – degrees & minutes...|\n",
      "| [43.486946, -79.38389]|        {{coord|43|29|13|N|79|23|02|W}}     43°29′4″N 79°23′0″W     Toronto – degrees, ...|\n",
      "|    [43.48683, -79.384]|        {{coord|43|29|12.6|N|79|23|02.4|W}}     43°29′12.6″N 79°23′02.4″W     Toronto –...|\n",
      "|  [55.75222, 37.615555]|        {{coord|55.752222|N|37.615556|E}}     55.752222°N 37.615556°E     Moscow – N & ...|\n",
      "|  [55.75222, 37.615555]|        {{coord|55.752222|N|37.615556|E|format=dms}}     55°45′08″N 37°36′56″E     Conv...|\n",
      "| [39.098095, -94.58731]|        {{coord|39.098095|-94.587307|format=dms}}     39°05′53″N 94°35′14″W     Decimal...|\n",
      "|  [55.75222, 37.615555]|        {{coord|55.752222|N|37.615556|E|format=dec|name=Moscow}}     55.752222°N 37.615...|\n",
      "|[-33.916668, 18.416666]|                    {{coord|33|55|S|18|25|E}}     33°55′S 18°25′E     Cape Town – S & E...|\n",
      "|          [35.0, 105.0]|         {{coord|35|00|N|105|00|E}}     35°00′N 105°00′E     People's Republic of China...|\n",
      "|[-22.908333, -43.24361]|              {{coord|22|54|30|S|43|14|37|W}}     22°54′30″S 43°14′37″W     Rio – S & W...|\n",
      "|         [-22.0, -43.0]|                             {{coord|22|S|43|W}}     22°S 43°W     A degree confluence....|\n",
      "|                   null|                  {{coord|LAT|LONG|display=inline,title}}  A common error on many pages...|\n",
      "|[52.466667, -1.9166666]|        {{coord|52|28|N|1|55|W|region:GB_type:city|notes=<ref>{{cite web|url=http://www...|\n",
      "| [51.430218, 0.7324167]|        {{coord|51|25.813|N|0|43.945|E}}     51°25.813′N 0°43.945′E     Navigation buoy...|\n",
      "| [51.604782, -8.533633]|        {{coord|51|36.287|N|8|32.018|W}}     51°36.287′N 8°32.018′W     Lighthouse at t...|\n",
      "|                   null|                                                                                       ...|\n",
      "+-----------------------+------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_coords = True\n",
    "if test_coords:\n",
    "    # https://en.wikipedia.org/wiki/Template:Coord\n",
    "    testcases = '''\n",
    "        {{coord|43.651234|-79.383333}}     43.651234°N 79.383333°W    Toronto – Fully decimal – western hemisphere implied by negation\n",
    "        {{coord|43.65|-79.38}}     43.65°N 79.38°W    Toronto – low precision decimal\n",
    "        {{coord|43.6500|-79.3800}}     43.6500°N 79.3800°W     Toronto – medium precision decimal with trailing zeroes\n",
    "        {{coord|43.653500|N|79.384000|W}}     43.653500°N 79.384000°W     Toronto – high precision decimal with explicit hemisphere notation\n",
    "        {{coord|43|29|N|79|23|W}}     43°29′N 79°23′W     Toronto – degrees & minutes\n",
    "        {{coord|43|29|13|N|79|23|02|W}}     43°29′4″N 79°23′0″W     Toronto – degrees, minutes & seconds\n",
    "        {{coord|43|29|12.6|N|79|23|02.4|W}}     43°29′12.6″N 79°23′02.4″W     Toronto – degrees, minutes, seconds & fractions of seconds\n",
    "        {{coord|55.752222|N|37.615556|E}}     55.752222°N 37.615556°E     Moscow – N & E\n",
    "        {{coord|55.752222|N|37.615556|E|format=dms}}     55°45′08″N 37°36′56″E     Convert to dms format\n",
    "        {{coord|39.098095|-94.587307|format=dms}}     39°05′53″N 94°35′14″W     Decimal conversion without N/S/E/W\n",
    "        {{coord|55.752222|N|37.615556|E|format=dec|name=Moscow}}     55.752222°N 37.615556°E     Convert to decimal and label on some maps\n",
    "        {{coord|33|55|S|18|25|E}}     33°55′S 18°25′E     Cape Town – S & E\n",
    "        {{coord|35|00|N|105|00|E}}     35°00′N 105°00′E     People's Republic of China\n",
    "        {{coord|22|54|30|S|43|14|37|W}}     22°54′30″S 43°14′37″W     Rio – S & W\n",
    "        {{coord|22|S|43|W}}     22°S 43°W     A degree confluence.\n",
    "        {{coord|LAT|LONG|display=inline,title}}  A common error on many pages\n",
    "        {{coord|52|28|N|1|55|W|region:GB_type:city|notes=<ref>{{cite web|url=http://www.fallingrain.com/world/UK/0/Birmingham.html|title=Birmingham}}</ref>|display=inline,title}}     52°28′N 1°55′W[1]Coordinates: 52°28′N 1°55′W[1]     Birmingham – with display, notes, and parameter settings; note that these coordinates are also displayed at the top of this page.\n",
    "        {{coord|51|25.813|N|0|43.945|E}}     51°25.813′N 0°43.945′E     Navigation buoy in the River Medway, England.\n",
    "        {{coord|51|36.287|N|8|32.018|W}}     51°36.287′N 8°32.018′W     Lighthouse at the Old Head of Kinsale as defined by the Commissioners of Irish Lights. \n",
    "    '''.splitlines()\n",
    "    spark.createDataFrame([[testcase] for testcase in testcases],'testcase string').createOrReplaceTempView('coord_testcases')\n",
    "    spark.sql('''select wikimedia_coordinates_udf(testcase) as latlon,substr(testcase,1,90)||'...' as testcase from coord_testcases''').show(40,90)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noble-health",
   "metadata": {},
   "source": [
    "#### Apply the mwparserfromhell parser on each document.\n",
    "\n",
    "* Work around memory-related limitations on tiny/cheap clusters (Azure's cheapest tier of Databricks workers, or a local desktop).\n",
    "\n",
    "The default`spark.sql.execution.arrow.maxRecordsPerBatch` of 10,000 doesn't work well when you hit ranges of some of the larger documents like\n",
    "\n",
    "    {\n",
    "       \"title\": \"Wikipedia:Arbitration Committee Elections December 2018/Coordination/Mass message\",\n",
    "       \"length(body)\": 4483016\n",
    "    },\n",
    "    {\n",
    "       \"title\": \"Wikipedia:CHECKWIKI/WPC 055 dump\",\n",
    "       \"length(body)\": 2472791\n",
    "    },\n",
    "\n",
    "and https://en.wikipedia.org/wiki/Help:Citation_Style_1/mass_test/fcite_web_ref\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "black-chemical",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "# even with 360 partitions, I get OOM errors\n",
    "# defaults to 10,000 which is painful on large wikipedia articles\n",
    "#\n",
    "# Especially ones like\n",
    "\n",
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\",1000)  \n",
    "#spark.conf.set(\"\",5)\n",
    "# 3% MEM at 125/341\n",
    "\n",
    "df1 = spark.sql(\"select * from wikipedia_bronze\")\n",
    "# Tricky workaround for memory leak in the UDFs defined above.\n",
    "#\n",
    "# Run against all documents in Wikipedia, it will leak (not use.. unnecessarily grow by)\n",
    "# about 40GB.  If spark splits this among 10 executors, each will grow by about 4GB, which\n",
    "# still exceeds the 32GB cheap cluster I'm using.   By splitting the dataframe into 10 batches,\n",
    "# it kills the \"python3 -m pyspark.daemon\" workers after each 10th of the data (so never\n",
    "# leaking over 4GB).  Using .repartition() won't have the same benefits because it seems \n",
    "# the task re-uses the \"python3 -m pyspark.daemon\" for multiple partitions.\n",
    "num_batches = 10\n",
    "df2s = [(df1.filter(f'((xxhash64(title)&9223372036854775807) % {num_batches}) = {i}')\n",
    "            .withColumn('parsed_elements',extract_mediawiki_info_udf('text')))\n",
    "        for i in range(num_batches)\n",
    "       ]\n",
    "df2 = functools.reduce(lambda x,y: x.union(y),df2s)\n",
    "df3 = df2.withColumn('parsed_elements',psf.from_json('parsed_elements',parsed_schema))\n",
    "df4 = df3.select('title','text','parsed_elements.*')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "engaged-fault",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will take about 3 hours on an 8-core (16-vcore) cluster; about 1 hour on a 48-vcore cluster.\n",
    "reparse_wikimedia_markup = False\n",
    "if reparse_wikimedia_markup:\n",
    "    spark.sql(\"drop table if exists wikipedia_silver_structured_templates\")\n",
    "    df4.write.format('delta').saveAsTable('wikipedia_silver_structured_templates')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stretch-adrian",
   "metadata": {},
   "source": [
    "## Spot check structured wikipedia data\n",
    "\n",
    "At this point we can query the infoboxes, coordinates, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "superb-implementation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(count(1)=21108360, count(DISTINCT title)=21108326)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('select count(*),count(distinct title) from  wikipedia_silver_structured_templates').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "imported-iraqi",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------+----------+\n",
      "|                         wikipedia_pages_with_multiple_coordinates|num_coords|\n",
      "+------------------------------------------------------------------+----------+\n",
      "|Wikipedia:WikiProject National Register of Historic Places/coordsH|      3692|\n",
      "|                     Sortable list of islands of Western Australia|      1139|\n",
      "|                                          List of schools in Perak|      1103|\n",
      "|                                      List of lakes of New Zealand|      1082|\n",
      "|                                     List of places in Colorado-02|      1077|\n",
      "|                                        List of places in Colorado|      1031|\n",
      "|                  List of cities, towns and villages in Gelderland|       842|\n",
      "|                                 List of waterfalls of Nova Scotia|       832|\n",
      "|       List of cities, towns and villages in Limburg (Netherlands)|       770|\n",
      "|                                      List of villages in Vestland|       765|\n",
      "+------------------------------------------------------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "  select title as wikipedia_pages_with_multiple_coordinates,\n",
    "         size(coords) as num_coords \n",
    "  from wikipedia_silver_structured_templates \n",
    "  order by size(coords) desc \n",
    "  limit 10\n",
    "''').show(100,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "mexican-washington",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------------------------------------------+--------------------------------------------------------------------------------+\n",
      "|count(1)|                                                 _error_|                                                             collect_list(title)|\n",
      "+--------+--------------------------------------------------------+--------------------------------------------------------------------------------+\n",
      "|   35007|                could not convert string to float: 'LAT'|[Andrea Pallaoro, Murder of Michele MacNeill, Henry J. Hendrix, Sarita Gurung...|\n",
      "|   24748|        Object of type Wikicode is not JSON serializable|[Usha Ananthasubramanian, Samsun Amisos Hill Gondola, Wikipedia:WikiProject W...|\n",
      "|    6927|                                                     '1'|[Casino Queen Marquette, Interdepartmental Working Group on Trafficking in Pe...|\n",
      "|    2546|                   could not convert string to float: ''|[Steilacoom Historical School District, Shamrock Hotel, Bendigo, Mount Carrol...|\n",
      "|    1173|           could not convert string to float: 'LATITUDE'|[Ministry of Finance (Nepal), Ministry of Peace and Reconstruction (Nepal), M...|\n",
      "|    1005|           could not convert string to float: 'latitude'|[High Shoals Creek Falls, Eden Valley 216, Teatro de la Victoria (Buenos Aire...|\n",
      "|     755|                  'NoneType' object is not subscriptable|[List of power stations in Togo, Kuhora Twana, Koyonzo, List of submarine top...|\n",
      "|     739|    could not convert string to float: 'source:wikidata'|[Katleri, Peanse, Kurnuvere, Rääka, Supsi, Võhmassaare, Ängi, Lätkalu, Parika...|\n",
      "|     425|                could not convert string to float: '...'|[Template:Article templates/River, Module:ISO 3166/sandbox, River Arrow, Wale...|\n",
      "|     365|            invalid literal for int() with base 10: 'DD'|[Kanjia Lake, Howchin Lake, Antoine Lake, Sawins Pond, Baskahegan Lake, Pavan...|\n",
      "|     179|could not convert string to float: 'region:KH_type:city'|[Chob Vari, Prasat, Preah Netr Preah, Ou Beichoan Commune, Phum Thmei, Chaeng...|\n",
      "|     135|       could not convert string to float: 'region:CA-QC'|[Lac-des-Écorces, Quebec (former unorganized territory), Saint-Jean-de-Matha,...|\n",
      "|     104|                 could not convert string to float: '..'|[Machai Hydropower Plant, Chanaka-Korata Barrage, Lake Sabula, Mărișelu Hydro...|\n",
      "|      88|       could not convert string to float: 'region:CA-ON'|[Convent Crash, Clifford, Ontario, Howick, Ontario, North Dumfries, Mulmur, N...|\n",
      "|      80|          could not convert string to float: 'region:EE'|[Põllu, Hüti, Võru County, Vilidu, Länga, Arandi, Lahetaguse, Anijala, Sõrve-...|\n",
      "|      50|         could not convert string to float: '{{{lat|}}}'|[Template:Kenya Monument row, Template:Nepal Monument row generic, Template:G...|\n",
      "|      44|              invalid literal for int() with base 10: ''|[Trams in Heidelberg, List of Methodist churches in the United States, List o...|\n",
      "|      26|                could not convert string to float: 'lat'|[Naic Church, Wikipedia:Templates for discussion/Log/2011 November 22, Manate...|\n",
      "|      26|                  could not convert string to float: 'n'|[List of church buildings in Philadelphia, Highlands Formation, Square of the...|\n",
      "|      22|      could not convert string to float: 'type:landmark'|[Cristo Rei of Dili, Draft:Sudersandsbiografen, Hull and Selby Railway, Templ...|\n",
      "+--------+--------------------------------------------------------+--------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"\"\"\n",
    "create table if not exists rows_with_errors as\n",
    "select title,_error_ from wikipedia_silver_structured_templates where _error_ is not null\n",
    "\"\"\")\n",
    "spark.sql(\"\"\"select count(*),_error_,collect_list(title) from rows_with_errors group by _error_ order by count(*) desc\"\"\").show(20,80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "coupled-sunday",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_large_articles = False\n",
    "if debug_large_articles:\n",
    "    sizes = spark.sql('''\n",
    "     select title,length(body) \n",
    "       from wikipedia_silver_structured_templates \n",
    "      where title not like 'Wikipedia:%' \n",
    "        and title not like 'Template:%' \n",
    "        and title not like 'Module:%' \n",
    "        and title not like 'MediaWiki:%'\n",
    "        and title not like 'Template:%' \n",
    "        and title not like 'Draft:%' \n",
    "        and title not like 'Help:%' \n",
    "        order by length(body) desc limit 10\n",
    "     ''').cache()\n",
    "    sizes.show(40,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "postal-florida",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|articles_with_coords|num_coords|\n",
      "+--------------------+----------+\n",
      "|             1203242|   1855143|\n",
      "+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''select count(*) as articles_with_coords,sum(size(coords)) as num_coords\n",
    "    from wikipedia_silver_structured_templates \n",
    "    where size(coords) > 0\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "widespread-cargo",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_h3</th>\n",
       "      <th>num_h4</th>\n",
       "      <th>num_h5</th>\n",
       "      <th>num_h6</th>\n",
       "      <th>num_h7</th>\n",
       "      <th>num_h8</th>\n",
       "      <th>num_h9</th>\n",
       "      <th>count(1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21799</td>\n",
       "      <td>72608</td>\n",
       "      <td>206136</td>\n",
       "      <td>498461</td>\n",
       "      <td>883727</td>\n",
       "      <td>1164699</td>\n",
       "      <td>1335738</td>\n",
       "      <td>1855143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_h3  num_h4  num_h5  num_h6  num_h7   num_h8   num_h9  count(1)\n",
       "0   21799   72608  206136  498461  883727  1164699  1335738   1855143"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# About a half million h6 grids with information in wikipedia.\n",
    "spark.sql('''\n",
    "  select count(distinct coord.h3[3]) as num_h3,\n",
    "         count(distinct coord.h3[4]) as num_h4,\n",
    "         count(distinct coord.h3[5]) as num_h5,\n",
    "         count(distinct coord.h3[6]) as num_h6,\n",
    "         count(distinct coord.h3[7]) as num_h7,\n",
    "         count(distinct coord.h3[8]) as num_h8,\n",
    "         count(distinct coord.h3[9]) as num_h9,\n",
    "         count(*)\n",
    " from wikipedia_silver_structured_templates\n",
    " lateral view explode(coords) as coord\n",
    "''').toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signal-penny",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "* http://mcburton.net/blog/static-files/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "charitable-indian",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+---------------------------------------------+\n",
      "|title                        |body                                         |\n",
      "+-----------------------------+---------------------------------------------+\n",
      "|Black panther                |{{short description|Melanistic colour va...  |\n",
      "|Great white shark            |{{other uses of|great white|Great White ...  |\n",
      "|Melisende, Queen of Jerusalem|{{short description|Queen regnant of the...  |\n",
      "|Carcharodon carcharias       |#REDIRECT [[Great white shark]] \\n \\n {{Re...|\n",
      "|Method                       |{{Wiktionary|method}} \\n '''Method''' ({{... |\n",
      "+-----------------------------+---------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(r\"select title,replace(substr(body,1,40)||'...','\\n','\\\\n') as body from wikipedia_bronze limit 5\").show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "distinct-findings",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='demo.html' target='_blank'>demo.html</a><br>"
      ],
      "text/plain": [
       "/home/jovyan/work/wikipedia_in_spark/notebooks/demo.html"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import FileLink, FileLinks\n",
    "FileLink('demo.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "muslim-astrology",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "Consider excluding documents tagged as \"unprintworthy\"\n",
    "\n",
    "    {{rcat shell|\n",
    "    {{R from move}}\n",
    "    {{R from CamelCase}}\n",
    "    {{R unprintworthy}}\n",
    "    }}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "accurate-voluntary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------+----------------------------------------+\n",
      "|                                  title|                                     dob|\n",
      "+---------------------------------------+----------------------------------------+\n",
      "|                       Jessica Cattaneo| {{birth date and age|1996|12|8|df=yes}}|\n",
      "|                            Erica Musso| {{birth date and age|1994|7|29|df=yes}}|\n",
      "|                             Ali Shawqi|    {{Birth date and age|2001|1|1|df=y}}|\n",
      "|                            Lisa Pigato| {{birth date and age|df=yes|2003|6|21}}|\n",
      "|                      Eduard Grandisson|                          {{circa|1798}}|\n",
      "|                         Jones Arogbofa|          {{Birth date|1952|11|10|dz=y}}|\n",
      "|                      Thiébaut Bischene|         {{birth date|1903|3|22|df=yes}}|\n",
      "|                           Nomsa Manaka|             {{Birth year and age|1962}}|\n",
      "|                            Josh Cahill| {{birth date and age|df=yes|1986|6|17}}|\n",
      "|                          Magnus Matter|         {{birth date|1898|4|28|df=yes}}|\n",
      "|                       George E. Abbott|                       November 20, 1858|\n",
      "|                               Yuma Edo|{{birth date and age|1993|11|30|df=yes}}|\n",
      "|                      Darrien Landsberg|   {{birth date and age|1998|7|26|df=y}}|\n",
      "|                         Martin Davídek|    {{birth date and age|1986|8|7|mf=y}}|\n",
      "|                       Tamaiti Williams|  {{birth date and age|2000|01|01|df=y}}|\n",
      "|                             Emily Syme|  {{birth date and age|2000|07|23|df=y}}|\n",
      "|              Michael Dickson (hurdler)|       {{Birth date and age|1994|01|25}}|\n",
      "|    José Guerra (footballer, born 1994)| {{birth date and age|1994|9|12|df=yes}}|\n",
      "|Shafiqul Islam (Kishoreganj politician)|                                        |\n",
      "|                        Víctor Griffith|{{birth date and age|2000|12|12|df=yes}}|\n",
      "+---------------------------------------+----------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select title,element_at(ib.params,'birth_date') as dob from wikipedia_silver_structured_templates\n",
    "lateral view explode(infoboxes) as ib\n",
    "where size(infoboxes)>0 \n",
    "and element_at(ib.params,'birth_date') is not null\n",
    "\"\"\").show(20,40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
