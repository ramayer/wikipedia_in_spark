{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sweet-webcam",
   "metadata": {},
   "source": [
    "# Index Wikipedia into Solr\n",
    "\n",
    "* Generate links like these: https://en.wikipedia.org/?curid=18630637 to avoid needing to canonicalize page names\n",
    "* Assumes Wikipedia has been pre-processed into delta tables as per the notebook [wikipedia_gis_analysis_with_h3_and_deckgl.ipynb](wikipedia_gis_analysis_with_h3_and_deckgl.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "induced-bouquet",
   "metadata": {},
   "outputs": [],
   "source": [
    "required_packages = {\"pysolr\",\"boltons\"}\n",
    "import pkg_resources\n",
    "for lib in required_packages - {pkg.key for pkg in pkg_resources.working_set}:\n",
    "    print(f\"installing {lib}\")\n",
    "    %pip install -q --upgrade pip\n",
    "    %pip install -q $lib\n",
    "    pkg_resources.require(lib)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "affiliated-found",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['SOLR_HOST'] = 'localhost'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "computational-retreat",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "solr_host       = os.environ.get('SOLR_HOST'      ,'solr')\n",
    "solr_collection = os.environ.get('SOLR_COLLECTION','wiki')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eight-hacker",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"responseHeader\":{\\n    \"zkConnected\":null,\\n    \"status\":0,\\n    \"QTime\":0,\\n    \"params\":{\\n      \"q\":\"{!lucene}*:*\",\\n      \"distrib\":\"false\",\\n      \"df\":\"_text_\",\\n      \"rows\":\"10\",\\n      \"echoParams\":\"all\",\\n      \"rid\":\"-3\"}},\\n  \"status\":\"OK\"}\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pysolr\n",
    "solr = pysolr.Solr(f'http://{solr_host}:8983/solr/{solr_collection}', always_commit=True)\n",
    "solr.ping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "clear-hartford",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://08651cc65d10:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>MyApp</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f83e954b0d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not \"spark\" in locals():\n",
    "    import pyspark\n",
    "    MAX_MEMORY = \"8g\"  # 24 gives OOM here. # 6 gives \"out of heap space\"\n",
    "    spark = (pyspark.sql.SparkSession.builder.appName(\"MyApp\") \n",
    "        .config(\"spark.jars.packages\"            , \"io.delta:delta-core_2.12:1.0.0\") \n",
    "        .config(\"spark.sql.extensions\"           , \"io.delta.sql.DeltaSparkSessionExtension\") \n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \n",
    "        .config(\"spark.executor.memory\"          , MAX_MEMORY) \n",
    "        .config(\"spark.driver.memory\"            , MAX_MEMORY) \n",
    "        .config(\"spark.python.worker.reuse\"      , False)\n",
    "        .config(\"spark.task.maxFailures\"         , 5)\n",
    "        .enableHiveSupport() \n",
    "        .getOrCreate()        \n",
    "        )\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "found-profile",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(count(1)=21108360, count(DISTINCT title)=21108326)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('select count(*),count(distinct title) from  wikipedia_silver_structured_templates').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "systematic-testament",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('select * from  wikipedia_silver_structured_templates limit 2').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shared-offset",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"\"\"select title,infoboxes from  wikipedia_silver_structured_templates where size(infoboxes)>0 limit 20\"\"\"\n",
    "print(json.dumps(spark.sql(s).take(1)[0].asDict(True),indent=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "involved-seeking",
   "metadata": {},
   "outputs": [],
   "source": [
    "solr_url = f'http://{solr_host}:8983/solr/{solr_collection}'\n",
    "from boltons.iterutils import remap\n",
    "import datetime\n",
    "\n",
    "class SolrForeachWriter:\n",
    "    def open(self, partition_id, epoch_id):\n",
    "        self._partition_id = partition_id\n",
    "        self._epoch_id     = epoch_id\n",
    "        self._batch_size   = 10000\n",
    "        self._solr_url     = solr_url\n",
    "        self.pending_docs  = []\n",
    "        return True\n",
    "        \n",
    "    def insert_pending_docs(self):\n",
    "        try:\n",
    "            if self.pending_docs == []: return\n",
    "            solr = pysolr.Solr(self._solr_url)\n",
    "            solr.add(self.pending_docs)\n",
    "            solr.commit()\n",
    "            self.pending_docs = []\n",
    "        except Exception as e:\n",
    "            errmsg = f\"{str(e)} adding {self.pending_docs}\"\n",
    "            raise\n",
    "            \n",
    "    def process(self,row):\n",
    "        def map_to_solr_types(path,key,value):\n",
    "            if value is None:\n",
    "                return False\n",
    "            if isinstance(value,datetime.datetime):\n",
    "                return key,value.isoformat()+\"Z\"\n",
    "            return key,value\n",
    "        solr_data = remap(row.asDict(),visit=map_to_solr_types)\n",
    "        self.pending_docs.append(solr_data)\n",
    "        if len(self.pending_docs) >= self._batch_size:\n",
    "            self.insert_pending_docs()\n",
    "        return True\n",
    "    \n",
    "    def close(self,error):\n",
    "        self.insert_pending_docs()\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "meaning-nigeria",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#spark.sql(\"\"\"DROP TABLE IF EXISTS wikipedia_for_solr\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "          CREATE TABLE IF NOT EXISTS wikipedia_for_solr (\n",
    "              id           string,\n",
    "              title_txt_en string,\n",
    "              body_txt_en  array<string>,\n",
    "              person_s     array<string>,\n",
    "              h3_1_s       array<string>,\n",
    "              h3_2_s       array<string>,\n",
    "              tags         array<string>\n",
    "          ) USING DELTA\n",
    "\"\"\")\n",
    "# df = spark.createDataFrame([],\"\"\"\n",
    "#               id           string,\n",
    "#               title_txt_en string,\n",
    "#               body_txt_en  array<string>,\n",
    "#               person_s     array<string>,\n",
    "#               h3_1_s       array<string>,\n",
    "#               h3_2_s       array<string>,\n",
    "#               tags         array<string>\n",
    "# \"\"\")\n",
    "# (df.write\n",
    "#    .format('delta')\n",
    "#    .mode('overwrite')\n",
    "#    .option(\"mergeSchema\", \"true\")\n",
    "#    .saveAsTable('try_3_wiki_solr', format='delta', mode='overwrite', path='file:/home/jovyan/work/wikipedia_in_spark/notebooks/spark-warehouse/tables/try_3_wiki_solr')\n",
    "#    #.saveAsTable('test_table', format='parquet', mode='overwrite', path='s3a://bucket/foo')\n",
    "#    #.save('./tables/try_2_wikipedia_for_solr')\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "catholic-cliff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(\"delete from wikipedia_for_solr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiac-belfast",
   "metadata": {},
   "source": [
    "Cute workaround for MERGE requiring matching columns...\n",
    "\n",
    "*  https://mungingdata.com/pyspark/union-unionbyname-merge-dataframes/\n",
    "\n",
    "\n",
    "Dedup to avoid\n",
    "\n",
    "Py4JJavaError: An error occurred while calling o43.sql.\n",
    ": java.lang.UnsupportedOperationException: Cannot perform Merge as multiple source rows matched and attempted to modify the same\n",
    "target row in the Delta table in possibly conflicting ways. By SQL semantics of Merge,\n",
    "when multiple source rows match on the same target row, the result may be ambiguous\n",
    "as it is unclear which source row should be used to update or delete the matching\n",
    "target row. You can preprocess the source table to eliminate the possibility of\n",
    "multiple matches. Please refer to\n",
    "https://docs.delta.io/latest/delta-update.html#upsert-into-a-table-using-mergePy4JJavaError: An error occurred while calling o43.sql.\n",
    ": java.lang.UnsupportedOperationException: Cannot perform Merge as multiple source rows matched and attempted to modify the same\n",
    "target row in the Delta table in possibly conflicting ways. By SQL semantics of Merge,\n",
    "when multiple source rows match on the same target row, the result may be ambiguous\n",
    "as it is unclear which source row should be used to update or delete the matching\n",
    "target row. You can preprocess the source table to eliminate the possibility of\n",
    "multiple matches. Please refer to\n",
    "https://docs.delta.io/latest/delta-update.html#upsert-into-a-table-using-merge\n",
    "\n",
    "\n",
    "https://docs.delta.io/latest/delta-update.html#-write-change-data-into-a-delta-table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ruled-shoulder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------+--------+------+------+----+\n",
      "|                  id|        title_txt_en|body_txt_en|person_s|h3_1_s|h3_2_s|tags|\n",
      "+--------------------+--------------------+-----------+--------+------+------+----+\n",
      "|+++2+g44nLlXebhn+...|              Abreus|       null|      []|  null|  null|null|\n",
      "|+++JdSu/cQmVT8S9O...|   About a Rib Chute|       null|      []|  null|  null|null|\n",
      "|+++bKtQ0tXJ0s4anJ...|Acontista cayenne...|       null|      []|  null|  null|null|\n",
      "|+++f0wez3y5KaqdMo...|         Androgynism|       null|      []|  null|  null|null|\n",
      "|+++ij94QGgnl5gc3F...|APRA Music Awards...|       null|      []|  null|  null|null|\n",
      "|++/4JcU4rSaPUVsMN...|Abdulla Fareed Al...|       null|      []|  null|  null|null|\n",
      "|++/O5H5Rd2/J/DoXD...|      ATI EGA Wonder|       null|      []|  null|  null|null|\n",
      "|++0+Jm6EVMNCq0e9k...|Amudala Palle, Pr...|       null|      []|  null|  null|null|\n",
      "|++0G+ZMipwJOpHVJt...|      Ahrayut Leumit|       null|      []|  null|  null|null|\n",
      "|++0Uw5iNGnQNKuUVi...|            Aurelian|       null|      []|  null|  null|null|\n",
      "|++1QYQW1izvQIiB21...|       Ahmaud arbery|       null|      []|  null|  null|null|\n",
      "|++1e1a/O18Pn/pSdj...|        Amphoe Khong|       null|      []|  null|  null|null|\n",
      "|++1qrmnGY3Fixkz/E...|Australorhynchus ...|       null|      []|  null|  null|null|\n",
      "|++1xk234fcqHG3oIF...|         Am. Ethnol.|       null|      []|  null|  null|null|\n",
      "|++2GDrnGhXwIvTxAY...|Antonio Maria de ...|       null|      []|  null|  null|null|\n",
      "|++2dN4c6Q9yhXdQGn...|       Amphoe Sichon|       null|      []|  null|  null|null|\n",
      "|++34mvwbNe3oXsCAG...|Aodh mac Maghnusa...|       null|      []|  null|  null|null|\n",
      "|++3bswMZWebenLZ/W...|               Ayomi|       null|      []|  null|  null|null|\n",
      "|++3wNu+D5eP/0lwhB...|  Alfred Baudrillart|       null|      []|  null|  null|null|\n",
      "|++3zBR22VNjkwAB3s...|António Thomas Sa...|       null|      []|  null|  null|null|\n",
      "+--------------------+--------------------+-----------+--------+------+------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "999998"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark.sql.functions as psf\n",
    "\n",
    "s = \"\"\"\n",
    "    SELECT\n",
    "       base64(sha2(title, 224))   as id,\n",
    "       title                             as title_txt_en,\n",
    "       --array(body)                     as body_txt_en,\n",
    "       array('')                         as person_s\n",
    "    FROM  wikipedia_silver_structured_templates\n",
    "    where lower(title) like 'a%' limit 1000000\n",
    "\"\"\"\n",
    "new_rows = (spark.sql(s)\n",
    "            .selectExpr(\"id\", \"struct(title_txt_en,person_s) as other_cols\")\n",
    "            .groupBy(\"id\")\n",
    "            .agg(psf.max(\"other_cols\").alias(\"latest\"))\n",
    "            .selectExpr(\"id\", \"latest.*\")\n",
    "           )\n",
    "new_rows = spark.sql('select * from wikipedia_for_solr limit 0').unionByName(new_rows,allowMissingColumns=True)\n",
    "new_rows.show()\n",
    "new_rows.createOrReplaceTempView('new_rows')\n",
    "new_rows.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "waiting-decade",
   "metadata": {},
   "outputs": [],
   "source": [
    "for active_stream in spark.streams.active:\n",
    "    print(f\"Stream: {active_stream.name} - {active_stream.explain()}\")\n",
    "    print(dir(active_stream))\n",
    "    #active_stream.stop()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "unnecessary-klein",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'responseHeader': {'status': 0,\n",
       "  'QTime': 61,\n",
       "  'params': {'q': 'title_txt_en:dog', 'fl': 'title_txt_en', 'wt': 'json'}},\n",
       " 'response': {'numFound': 9096,\n",
       "  'start': 0,\n",
       "  'numFoundExact': True,\n",
       "  'docs': [{'title_txt_en': 'Dogs Is Dogs'},\n",
       "   {'title_txt_en': 'Dogs is Dogs'},\n",
       "   {'title_txt_en': 'The Dogs'},\n",
       "   {'title_txt_en': 'There is no dog'},\n",
       "   {'title_txt_en': 'Dog'},\n",
       "   {'title_txt_en': 'DOG'},\n",
       "   {'title_txt_en': 'Dogs'},\n",
       "   {'title_txt_en': 'That Dog'},\n",
       "   {'title_txt_en': 'That Dog.'},\n",
       "   {'title_txt_en': 'Dogging'}]}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solr.search(\n",
    "    **{'q':'title_txt_en:dog','fl':'title_txt_en'}\n",
    "           ).raw_response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "extraordinary-plate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "| 1000000|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(*) from wikipedia_for_solr\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "front-packing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(\"describe extended wikipedia_for_solr\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "blank-civilian",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(\"select size(body_txt_en) from new_rows limit(1)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "absent-helicopter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    MERGE INTO wikipedia_for_solr\n",
    "    USING new_rows\n",
    "      ON new_rows.id = wikipedia_for_solr.id\n",
    "    WHEN MATCHED THEN UPDATE SET *\n",
    "    WHEN NOT MATCHED THEN INSERT *\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "industrial-liver",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select * from wikipedia_for_solr\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "included-tract",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.readStream.format(\"delta\").option(\"ignoreChanges\",True).table(\"wikipedia_for_solr\")\n",
    "#df = spark.readStream.format(\"delta\").option(\"ignoreChanges\",True).load('./tables/try_2_wikipedia_for_solr')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "analyzed-compact",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "rational-trinidad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '/tmp/new_rows_checkpoint4': Is a directory\n"
     ]
    }
   ],
   "source": [
    "!rm /tmp/new_rows_checkpoint4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "blank-twelve",
   "metadata": {},
   "outputs": [],
   "source": [
    "sq = sdf.writeStream \\\n",
    "  .trigger(once=True) \\\n",
    "  .foreach(SolrForeachWriter()) \\\n",
    "  .option('checkpointLocation','/tmp/new_rows_checkpoint4') \\\n",
    "  .outputMode(\"update\") \\\n",
    "  .start()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "least-latter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "WriteToDataSourceV2 org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@4713031f\n",
      "+- FileScan parquet [id#3777,title_txt_en#3778,body_txt_en#3779,person_s#3780,h3_1_s#3781,h3_2_s#3782,tags#3783] Batched: false, DataFilters: [], Format: Parquet, Location: TahoeBatchFileIndex[file:/home/jovyan/work/wikipedia_in_spark/notebooks/spark-warehouse/wikipedia..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:string,title_txt_en:string,body_txt_en:array<string>,person_s:array<string>,h3_1_s:arra...\n",
      "\n",
      "\n",
      "At 0.008057176135480404 stream: None - None\n",
      "== Physical Plan ==\n",
      "WriteToDataSourceV2 org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@4713031f\n",
      "+- FileScan parquet [id#3777,title_txt_en#3778,body_txt_en#3779,person_s#3780,h3_1_s#3781,h3_2_s#3782,tags#3783] Batched: false, DataFilters: [], Format: Parquet, Location: TahoeBatchFileIndex[file:/home/jovyan/work/wikipedia_in_spark/notebooks/spark-warehouse/wikipedia..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:string,title_txt_en:string,body_txt_en:array<string>,person_s:array<string>,h3_1_s:arra...\n",
      "\n",
      "\n",
      "At 10.03176300204359 stream: None - None\n",
      "== Physical Plan ==\n",
      "WriteToDataSourceV2 org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@4713031f\n",
      "+- FileScan parquet [id#3777,title_txt_en#3778,body_txt_en#3779,person_s#3780,h3_1_s#3781,h3_2_s#3782,tags#3783] Batched: false, DataFilters: [], Format: Parquet, Location: TahoeBatchFileIndex[file:/home/jovyan/work/wikipedia_in_spark/notebooks/spark-warehouse/wikipedia..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:string,title_txt_en:string,body_txt_en:array<string>,person_s:array<string>,h3_1_s:arra...\n",
      "\n",
      "\n",
      "At 20.04516199720092 stream: None - None\n",
      "Done at 30.056023654993623\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t0 = time.perf_counter()\n",
    "while True:\n",
    "    astreams = spark.streams.active \n",
    "    if len(astreams)>0:\n",
    "        print(f\"At {time.perf_counter() - t0} stream: {astreams[0].name} - {astreams[0].explain()}\")\n",
    "        time.sleep(10)\n",
    "    else:\n",
    "        print(f\"Done at {time.perf_counter() - t0}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powered-failure",
   "metadata": {},
   "source": [
    "### Verify that it worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "opposed-clone",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'responseHeader': {'status': 0,\n",
       "  'QTime': 38,\n",
       "  'params': {'q': 'timestamp:[NOW-1HOUR TO NOW]',\n",
       "   'fl': 'timestamp,title_txt_en,body_txt_en',\n",
       "   'wt': 'json'}},\n",
       " 'response': {'numFound': 999998,\n",
       "  'start': 0,\n",
       "  'numFoundExact': True,\n",
       "  'docs': [{'title_txt_en': 'Arthur Napoleon Raymond Robinson (airport)',\n",
       "    'timestamp': '2021-06-07T10:01:59.506Z'},\n",
       "   {'title_txt_en': 'Arts Council~Haliburton Highlands',\n",
       "    'timestamp': '2021-06-07T10:01:59.506Z'},\n",
       "   {'title_txt_en': 'Asago', 'timestamp': '2021-06-07T10:01:59.506Z'},\n",
       "   {'title_txt_en': 'Appropriation Act 2007',\n",
       "    'timestamp': '2021-06-07T10:01:59.209Z'},\n",
       "   {'title_txt_en': 'Analog Gut', 'timestamp': '2021-06-07T10:01:59.506Z'},\n",
       "   {'title_txt_en': 'Anthidium deceptum',\n",
       "    'timestamp': '2021-06-07T10:01:59.209Z'},\n",
       "   {'title_txt_en': 'Assyrian war of independence',\n",
       "    'timestamp': '2021-06-07T10:01:59.506Z'},\n",
       "   {'title_txt_en': 'Attar railway station',\n",
       "    'timestamp': '2021-06-07T10:01:59.209Z'},\n",
       "   {'title_txt_en': 'Agreement for the Mutual Safeguarding of Secrecy of Inventions relating to Defence and for which Applications for Patents have been made',\n",
       "    'timestamp': '2021-06-07T10:01:59.506Z'},\n",
       "   {'title_txt_en': 'Autodesk lustre',\n",
       "    'timestamp': '2021-06-07T10:01:59.209Z'}]}}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solr.search(\n",
    "    **{'q':'timestamp:[NOW-1HOUR TO NOW]','fl':'timestamp,title_txt_en,body_txt_en'}\n",
    ").raw_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dense-shanghai",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'val': '2021-06-07T10:01:59Z', 'count': 159999},\n",
       " {'val': '2021-06-07T10:02:03Z', 'count': 159999},\n",
       " {'val': '2021-06-07T10:02:06Z', 'count': 144267},\n",
       " {'val': '2021-06-07T10:02:07Z', 'count': 10000},\n",
       " {'val': '2021-06-07T10:02:10Z', 'count': 150000},\n",
       " {'val': '2021-06-07T10:02:12Z', 'count': 150000},\n",
       " {'val': '2021-06-07T10:02:14Z', 'count': 10000},\n",
       " {'val': '2021-06-07T10:02:15Z', 'count': 140000},\n",
       " {'val': '2021-06-07T10:02:17Z', 'count': 75733}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "solr_host='192.168.12.110'\n",
    "solr_collection='wiki'\n",
    "solr_facet_query = {\n",
    "    \"query\": \"*:*\",\n",
    "    \"limit\": 0,\n",
    "    \"facet\": {\n",
    "        \"docs_per_minute\": {\n",
    "            \"type\":  \"range\",\n",
    "            \"field\": \"timestamp\",\n",
    "            \"start\": \"NOW/HOUR-10MINUTES\",\n",
    "            \"end\":   \"NOW\",\n",
    "            \"gap\":   \"+1SECOND\",\n",
    "            \"other\": \"all\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "headers = {'Content-type':'application/json'}\n",
    "resp = requests.post(f\"\"\"http://{solr_host}:8983/solr/{solr_collection}/select?q=*:*\"\"\",\n",
    "                 json = solr_facet_query,\n",
    "                 headers = headers)\n",
    "[b for b in resp.json()['facets']['docs_per_minute']['buckets'] if b['count']> 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attractive-insertion",
   "metadata": {},
   "source": [
    "\n",
    "### Notes\n",
    "\n",
    "* With \"io.delta:delta-core_2.12:0.8.0\", the steps abovegave errors like\n",
    "  `AnalysisException: Table default.wikipedia_for_solr does not support either micro-batch or continuous scan.;`\n",
    "\n",
    "* A workaround was to create both the streaming sources and the table merging into them like this: \n",
    "`.saveAsTable('try_3_wiki_solr', format='delta', mode='overwrite', path='file:/home/jovyan/work/wikipedia_in_spark/notebooks/spark-warehouse/tables/try_3_wiki_solr')`\n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "### workaround this: \n",
    "\n",
    "* https://github.com/delta-io/delta/issues/594\n",
    "* https://stackoverflow.com/questions/66106096/delta-lake-insert-into-sql-in-pyspark-is-failing-with-java-lang-nosuchmethoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepting-morning",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
