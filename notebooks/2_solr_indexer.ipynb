{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "infinite-makeup",
   "metadata": {},
   "source": [
    "# Index Wikipedia into Solr\n",
    "\n",
    "* Assumes Wikipedia has been pre-processed into delta tables as per the notebook [wikipedia_gis_analysis_with_h3_and_deckgl.ipynb](wikipedia_gis_analysis_with_h3_and_deckgl.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "perceived-lounge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "installing pysolr\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "installing mwparserfromhell\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "installing boltons\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "required_packages = {\"mwparserfromhell\",\"pysolr\",\"boltons\"}\n",
    "\n",
    "import pkg_resources\n",
    "for lib in required_packages - {pkg.key for pkg in pkg_resources.working_set}:\n",
    "    print(f\"installing {lib}\")\n",
    "    %pip install -q --upgrade pip\n",
    "    %pip install -q $lib\n",
    "    pkg_resources.require(lib)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cellular-commander",
   "metadata": {},
   "outputs": [],
   "source": [
    "solr_host = '192.168.12.110'\n",
    "solr_collection = 'wiki'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "contrary-international",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"responseHeader\":{\\n    \"zkConnected\":null,\\n    \"status\":0,\\n    \"QTime\":2,\\n    \"params\":{\\n      \"q\":\"{!lucene}*:*\",\\n      \"distrib\":\"false\",\\n      \"df\":\"_text_\",\\n      \"rows\":\"10\",\\n      \"echoParams\":\"all\",\\n      \"rid\":\"-1\"}},\\n  \"status\":\"OK\"}\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pysolr\n",
    "solr = pysolr.Solr(f'http://{solr_host}:8983/solr/{solr_collection}', always_commit=True)\n",
    "solr.ping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "technological-apache",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://5110d7fddff0:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>MyApp</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f65c261c880>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not \"spark\" in locals():\n",
    "    import pyspark\n",
    "    MAX_MEMORY = \"8g\"  # 24 gives OOM here. # 6 gives \"out of heap space\"\n",
    "    spark = (pyspark.sql.SparkSession.builder.appName(\"MyApp\") \n",
    "        .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:0.8.0\") \n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \n",
    "        .config(\"spark.executor.memory\", MAX_MEMORY) \n",
    "        .config(\"spark.driver.memory\", MAX_MEMORY) \n",
    "        .config(\"spark.python.worker.reuse\",False)\n",
    "        .config(\"spark.task.maxFailures\",5)\n",
    "        .enableHiveSupport() \n",
    "        .getOrCreate()        \n",
    "        )\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "spanish-honey",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(count(1)=21108360, count(DISTINCT title)=21108326)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('select count(*),count(distinct title) from  wikipedia_silver_structured_templates').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "activated-first",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- title: string (nullable = true)\n",
      " |-- body: string (nullable = true)\n",
      " |-- infoboxes: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- params: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |    |-- body: string (nullable = true)\n",
      " |-- templates: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- params: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |    |-- body: string (nullable = true)\n",
      " |-- extlinks: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- title: string (nullable = true)\n",
      " |    |    |-- url: string (nullable = true)\n",
      " |-- wikilinks: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- text: string (nullable = true)\n",
      " |    |    |-- title: string (nullable = true)\n",
      " |-- coords: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- lat: float (nullable = true)\n",
      " |    |    |-- lon: float (nullable = true)\n",
      " |    |    |-- h3: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |-- _error_: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from  wikipedia_silver_structured_templates limit 2').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "removed-occupation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+--------------------------------------------------+\n",
      "|                                             title|                                         infoboxes|\n",
      "+--------------------------------------------------+--------------------------------------------------+\n",
      "|                                  Jessica Cattaneo|[{Infobox swimmer, {name -> Jessica Cattaneo, i...|\n",
      "|                         Category:Monza Rally Show|                                                []|\n",
      "|                                    Royal fortress|                                                []|\n",
      "|                                       Erica Musso|[{Infobox swimmer, {name -> Erica Musso, image ...|\n",
      "|                             Wallace Edwin Sturgis|                                                []|\n",
      "|                       Prêmio Jabuti de Literatura|                                                []|\n",
      "|                                          Totbrief|                                                []|\n",
      "|                         Paraguay-Turkey relations|                                                []|\n",
      "|                                      Dot McKinnon|                                                []|\n",
      "|                               Arushi Nishank Pant|                                                []|\n",
      "|                                             Brach|                                                []|\n",
      "|                                        Ali Shawqi|[{Infobox football biography, {name -> Ali Shaw...|\n",
      "|                           Kayseri Gençler Birliği|[{Infobox football club, {clubname -> Kayseri G...|\n",
      "|                         Haryana Electricity Board|                                                []|\n",
      "|Category:Christian organizations disestablished...|                                                []|\n",
      "|Flesh  &  Blood (Invictus Games Choir and Garet...|[{Infobox song, {name -> Flesh  &  Blood, cover...|\n",
      "|               North of Scotland Electricity Board|                                                []|\n",
      "|                                       Lisa Pigato|[{Infobox tennis biography, {name -> Lisa Pigat...|\n",
      "|                                    Mane Number 13|[{Infobox film, {name -> Mane Number 13, image ...|\n",
      "|                       A Fortune for Your Disaster|                                                []|\n",
      "+--------------------------------------------------+--------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select title,infoboxes from  wikipedia_silver_structured_templates limit 20').show(20,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "operational-lottery",
   "metadata": {},
   "outputs": [],
   "source": [
    "solr_url = f'http://{solr_host}:8983/solr/{solr_collection}'\n",
    "from boltons.iterutils import remap\n",
    "import datetime\n",
    "\n",
    "class SolrForeachWriter:\n",
    "    def open(self, partition_id, epoch_id):\n",
    "        self._partition_id = partition_id\n",
    "        self._epoch_id     = epoch_id\n",
    "        self._batch_size   = 10000\n",
    "        self._solr_url     = solr_url\n",
    "        self.pending_docs  = []\n",
    "        return True\n",
    "        \n",
    "    def insert_pending_docs(self):\n",
    "        try:\n",
    "            if self.pending_docs == []: return\n",
    "            solr = pysolr.Solr(self._solr_url)\n",
    "            solr.add(self.pending_docs)\n",
    "            solr.commit()\n",
    "            self.pending_docs = []\n",
    "        except Exception as e:\n",
    "            errmsg = f\"{str(e)} adding {self.pending_docs}\"\n",
    "            raise\n",
    "            \n",
    "    def process(self,row):\n",
    "        def map_to_solr_types(path,key,value):\n",
    "            if value is None:\n",
    "                return False\n",
    "            if isinstance(value,datetime.datetime):\n",
    "                return key,value.isoformat()+\"Z\"\n",
    "            return key,value\n",
    "        solr_data = remap(row.asDict(),visit=map_to_solr_types)\n",
    "        self.pending_docs.append(solr_data)\n",
    "        if len(self.pending_docs) >= self._batch_size:\n",
    "            self.insert_pending_docs()\n",
    "        return True\n",
    "    \n",
    "    def close(self,error):\n",
    "        self.insert_pending_docs()\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "eight-space",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(\"\"\"DROP TABLE IF EXISTS wikipedia_for_solr\"\"\")\n",
    "# spark.sql(\"\"\"\n",
    "#           CREATE TABLE IF NOT EXISTS wikipedia_for_solr (\n",
    "#               id           string,\n",
    "#               title_txt_en string,\n",
    "#               body_txt_en  array<string>,\n",
    "#               person_s     array<string>,\n",
    "#               h3_1_s       array<string>,\n",
    "#               h3_2_s       array<string>,\n",
    "#               tags         array<string>\n",
    "#           ) USING DELTA\n",
    "# \"\"\")\n",
    "###  gives errors like\n",
    "###     AnalysisException: Table default.wikipedia_for_solr does not support either micro-batch or continuous scan.;\n",
    "###  when used as a streaming source.\n",
    "\n",
    "df = spark.createDataFrame([],\"\"\"\n",
    "              id           string,\n",
    "              title_txt_en string,\n",
    "              body_txt_en  array<string>,\n",
    "              person_s     array<string>,\n",
    "              h3_1_s       array<string>,\n",
    "              h3_2_s       array<string>,\n",
    "              tags         array<string>\n",
    "\"\"\")\n",
    "(df.write\n",
    "   .format('delta')\n",
    "   .mode('overwrite')\n",
    "   .option(\"mergeSchema\", \"true\")\n",
    "   .save('./tables/try_2_wikipedia_for_solr')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desperate-outline",
   "metadata": {},
   "source": [
    "Cute workaround for MERGE requiring matching columns...\n",
    "\n",
    "*  https://mungingdata.com/pyspark/union-unionbyname-merge-dataframes/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "faced-ridge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------+--------+------+------+----+\n",
      "|                  id|        title_txt_en|body_txt_en|person_s|h3_1_s|h3_2_s|tags|\n",
      "+--------------------+--------------------+-----------+--------+------+------+----+\n",
      "|0cDExrekUzzUnblPC...|            True dog|       null|      []|  null|  null|null|\n",
      "|zXtrS7/VTZPHXnqvL...|2010–11 Minnesota...|       null|      []|  null|  null|null|\n",
      "|pJafRXTBkEaAhZS7l...|2019-20 Minnesota...|       null|      []|  null|  null|null|\n",
      "|EKfAaB6845wlWPQ9E...|File:The Dog Star...|       null|      []|  null|  null|null|\n",
      "|50ndknKe8lgvu2bNN...|Category:Wingate ...|       null|      []|  null|  null|null|\n",
      "|xsohacu8rfUbslR7O...|Category:Gardner–...|       null|      []|  null|  null|null|\n",
      "|0Y2kKLJ4NZpkLBGJv...|Category:Louisian...|       null|      []|  null|  null|null|\n",
      "|ZYpIfJZ4cs/Rs7/IV...|Category:Alabama ...|       null|      []|  null|  null|null|\n",
      "|7cz83066VzyfR4LsN...|2003–04 Butler Bu...|       null|      []|  null|  null|null|\n",
      "|HMbbX1NciGBEPrueB...|File:Dogs On Acid...|       null|      []|  null|  null|null|\n",
      "|THR97MbjcozYSKJYv...|Category:Bungo St...|       null|      []|  null|  null|null|\n",
      "|1JJB+CvSo4LG6tEyY...|Dogmatic falsific...|       null|      []|  null|  null|null|\n",
      "|scDpkIZqMKmicA8DR...|Dog Creek (Osage ...|       null|      []|  null|  null|null|\n",
      "|Bj2B+OjT5+KneTr/3...|Stray Dog (disamb...|       null|      []|  null|  null|null|\n",
      "|EQGvkowZLh0rFyJm/...|          Rescue Dog|       null|      []|  null|  null|null|\n",
      "|GFE65uxzEEw9qAldx...|           Dog Tired|       null|      []|  null|  null|null|\n",
      "|/1NJPgjwLyJk+dqdy...|Anaspidoglanis akiri|       null|      []|  null|  null|null|\n",
      "|Il2fNNKxGT18hzAzX...|Wikipedia:Sockpup...|       null|      []|  null|  null|null|\n",
      "|5ErBIDlpjGnILoGZ4...|Template:Taxonomy...|       null|      []|  null|  null|null|\n",
      "|fCYsbl+U6QjjMhVZ3...|Go, Dog, Go! (TV ...|       null|      []|  null|  null|null|\n",
      "+--------------------+--------------------+-----------+--------+------+------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_rows = spark.sql(\"\"\"\n",
    "  select base64(sha2(title, 224)) as id,\n",
    "  title as title_txt_en,\n",
    "  array('') as person_s\n",
    "  from  wikipedia_silver_structured_templates\n",
    "  where lower(title) like '%dog%' limit 1000000\n",
    "\"\"\")\n",
    "new_rows = spark.sql('select * from wikipedia_for_solr limit 0').unionByName(new_rows,allowMissingColumns=True)\n",
    "new_rows.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "accessory-aberdeen",
   "metadata": {},
   "outputs": [],
   "source": [
    "(new_rows.write\n",
    "   .format('delta')\n",
    "   .mode('overwrite')\n",
    "   .option(\"mergeSchema\", \"true\")\n",
    "   .save('/tmp/new_rows')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "personalized-granny",
   "metadata": {},
   "outputs": [],
   "source": [
    "for active_stream in spark.streams.active:\n",
    "    print(f\"Stream: {active_stream.name} - {active_stream.explain()}\")\n",
    "    print(dir(active_stream))\n",
    "    #active_stream.stop()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "internal-vermont",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stream2 = spark.readStream.format(\"delta\").option(\"ignoreChanges\",True).load(\"/tmp/new_rows\")\n",
    "stream2.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "elder-telephone",
   "metadata": {},
   "outputs": [],
   "source": [
    "sq = stream2.writeStream \\\n",
    "  .trigger(once=True) \\\n",
    "  .foreach(SolrForeachWriter()) \\\n",
    "  .option('checkpointLocation','/tmp/new_rows_checkpoint3') \\\n",
    "  .outputMode(\"update\") \\\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perfect-macro",
   "metadata": {},
   "source": [
    "#### workaround this: \n",
    "\n",
    "* https://github.com/delta-io/delta/issues/594\n",
    "* https://stackoverflow.com/questions/66106096/delta-lake-insert-into-sql-in-pyspark-is-failing-with-java-lang-nosuchmethoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "rolled-flooring",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'responseHeader': {'status': 0,\n",
       "  'QTime': 0,\n",
       "  'params': {'q': 'title_txt_en:cat', 'fl': 'title_txt_en', 'wt': 'json'}},\n",
       " 'response': {'numFound': 536,\n",
       "  'start': 0,\n",
       "  'numFoundExact': True,\n",
       "  'docs': [{'title_txt_en': 'Cats'},\n",
       "   {'title_txt_en': 'Black cat, White cat'},\n",
       "   {'title_txt_en': 'Cat Roberts'},\n",
       "   {'title_txt_en': 'Cats and the Fiddle'},\n",
       "   {'title_txt_en': 'Cunning Cat'},\n",
       "   {'title_txt_en': 'Gypsy  &  the Cat'},\n",
       "   {'title_txt_en': 'Cat Doucet'},\n",
       "   {'title_txt_en': 'Oscar the cat'},\n",
       "   {'title_txt_en': 'Cat Boyd'},\n",
       "   {'title_txt_en': \"The Cat's Bah\"}]}}"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solr.search(\n",
    "    **{'q':'title_txt_en:cat','fl':'title_txt_en'}\n",
    "           ).raw_response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "adolescent-threshold",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<response>\\n\\n<lst name=\"responseHeader\">\\n  <int name=\"status\">0</int>\\n  <int name=\"QTime\">120</int>\\n</lst>\\n</response>\\n'"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solr.delete(q='*:*')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animal-dayton",
   "metadata": {},
   "source": [
    "## Merge into is broken with Solr 3.1.1 and Delta.io 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mexican-declaration",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boxed-attention",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "temporal-transsexual",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Table not found: delta.tables/try_2_wikipedia_for_solr;\n'InsertIntoStatement 'UnresolvedRelation [delta, tables/try_2_wikipedia_for_solr], [], false, false, false\n+- Project [id#20292, title_txt_en#20293, body_txt_en#20294, person_s#20295, h3_1_s#20296, h3_2_s#20297, tags#20298]\n   +- SubqueryAlias new_rows\n      +- Union false, false\n         :- GlobalLimit 0\n         :  +- LocalLimit 0\n         :     +- Project [id#20292, title_txt_en#20293, body_txt_en#20294, person_s#20295, h3_1_s#20296, h3_2_s#20297, tags#20298]\n         :        +- SubqueryAlias spark_catalog.default.wikipedia_for_solr\n         :           +- Relation[id#20292,title_txt_en#20293,body_txt_en#20294,person_s#20295,h3_1_s#20296,h3_2_s#20297,tags#20298] parquet\n         +- Project [id#20278, title_txt_en#20279, null AS body_txt_en#20306, person_s#20280, null AS h3_1_s#20307, null AS h3_2_s#20308, null AS tags#20309]\n            +- GlobalLimit 2\n               +- LocalLimit 2\n                  +- Project [base64(cast(sha2(cast(title#20281 as binary), 224) as binary)) AS id#20278, title#20281 AS title_txt_en#20279, array(George Washington) AS person_s#20280]\n                     +- SubqueryAlias spark_catalog.default.wikipedia_silver_structured_templates\n                        +- Relation[title#20281,body#20282,infoboxes#20283,templates#20284,extlinks#20285,wikilinks#20286,coords#20287,_error_#20288] parquet\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-125-f4781c14d52b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mDELETE\u001b[0m \u001b[0mFROM\u001b[0m \u001b[0mwikipedia_for_solr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \"\"\")\n\u001b[0;32m---> 22\u001b[0;31m     spark.sql(\"\"\"\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mINSERT\u001b[0m \u001b[0mINTO\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtables\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtry_2_wikipedia_for_solr\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mSELECT\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mFrom\u001b[0m \u001b[0mnew_rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m         \"\"\"\n\u001b[0;32m--> 723\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    724\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtableName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Table not found: delta.tables/try_2_wikipedia_for_solr;\n'InsertIntoStatement 'UnresolvedRelation [delta, tables/try_2_wikipedia_for_solr], [], false, false, false\n+- Project [id#20292, title_txt_en#20293, body_txt_en#20294, person_s#20295, h3_1_s#20296, h3_2_s#20297, tags#20298]\n   +- SubqueryAlias new_rows\n      +- Union false, false\n         :- GlobalLimit 0\n         :  +- LocalLimit 0\n         :     +- Project [id#20292, title_txt_en#20293, body_txt_en#20294, person_s#20295, h3_1_s#20296, h3_2_s#20297, tags#20298]\n         :        +- SubqueryAlias spark_catalog.default.wikipedia_for_solr\n         :           +- Relation[id#20292,title_txt_en#20293,body_txt_en#20294,person_s#20295,h3_1_s#20296,h3_2_s#20297,tags#20298] parquet\n         +- Project [id#20278, title_txt_en#20279, null AS body_txt_en#20306, person_s#20280, null AS h3_1_s#20307, null AS h3_2_s#20308, null AS tags#20309]\n            +- GlobalLimit 2\n               +- LocalLimit 2\n                  +- Project [base64(cast(sha2(cast(title#20281 as binary), 224) as binary)) AS id#20278, title#20281 AS title_txt_en#20279, array(George Washington) AS person_s#20280]\n                     +- SubqueryAlias spark_catalog.default.wikipedia_silver_structured_templates\n                        +- Relation[title#20281,body#20282,infoboxes#20283,templates#20284,extlinks#20285,wikilinks#20286,coords#20287,_error_#20288] parquet\n"
     ]
    }
   ],
   "source": [
    "new_rows.createOrReplaceTempView(\"new_rows\")\n",
    "buggy_spark = True\n",
    "if not buggy_spark:\n",
    "    spark.sql(\"\"\"\n",
    "        create table tmp_new_rows_tbl using delta as select * from new_rows\n",
    "    \"\"\")\n",
    "    spark.sql(\"\"\"\n",
    "        MERGE INTO delta.`tables/try_2_wikipedia_for_solr`\n",
    "        USING tmp_new_rows_tbl\n",
    "          ON new_rows.id = wikipedia_for_solr.id\n",
    "        WHEN MATCHED THEN UPDATE SET *\n",
    "        WHEN NOT MATCHED THEN INSERT *\n",
    "    \"\"\")\n",
    "else:\n",
    "    spark.sql(\"\"\"\n",
    "        drop table if exists tmp_new_rows_tbl\n",
    "    \"\"\")\n",
    "    spark.sql(\"\"\"\n",
    "        DELETE FROM wikipedia_for_solr \n",
    "    \"\"\")\n",
    "    spark.sql(\"\"\"\n",
    "        INSERT INTO delta.`tables/try_2_wikipedia_for_solr`\n",
    "        SELECT * From new_rows\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "engaging-fantasy",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Table default.wikipedia_for_solr does not support either micro-batch or continuous scan.;\nSubqueryAlias spark_catalog.default.wikipedia_for_solr\n+- StreamingRelationV2 default.wikipedia_for_solr, DeltaTableV2(org.apache.spark.sql.SparkSession@5c128f74,file:/home/jovyan/work/wikipedia_in_spark/notebooks/spark-warehouse/wikipedia_for_solr,Some(CatalogTable(\nDatabase: default\nTable: wikipedia_for_solr\nOwner: jovyan\nCreated Time: Mon May 24 04:10:12 UTC 2021\nLast Access: UNKNOWN\nCreated By: Spark 3.1.1\nType: MANAGED\nProvider: delta\nLocation: file:/home/jovyan/work/wikipedia_in_spark/notebooks/spark-warehouse/wikipedia_for_solr\nSerde Library: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\nInputFormat: org.apache.hadoop.mapred.SequenceFileInputFormat\nOutputFormat: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\nPartition Provider: Catalog)),Some(default.wikipedia_for_solr),None,org.apache.spark.sql.util.CaseInsensitiveStringMap@1f), [ignoreChanges=true], [id#19790, title_txt_en#19791, body_txt_en#19792, person_s#19793, h3_1_s#19794, h3_2_s#19795, tags#19796], org.apache.spark.sql.delta.catalog.DeltaCatalog@450f4666, default.wikipedia_for_solr\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-1227c803ae30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadStream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"delta\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignoreChanges\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"wikipedia_for_solr\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadStream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"delta\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignoreChanges\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./tables/try_2_wikipedia_for_solr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mtable\u001b[0;34m(self, tableName)\u001b[0m\n\u001b[1;32m    978\u001b[0m         \"\"\"\n\u001b[1;32m    979\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtableName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 980\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtableName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    981\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tableName can be only a single string\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Table default.wikipedia_for_solr does not support either micro-batch or continuous scan.;\nSubqueryAlias spark_catalog.default.wikipedia_for_solr\n+- StreamingRelationV2 default.wikipedia_for_solr, DeltaTableV2(org.apache.spark.sql.SparkSession@5c128f74,file:/home/jovyan/work/wikipedia_in_spark/notebooks/spark-warehouse/wikipedia_for_solr,Some(CatalogTable(\nDatabase: default\nTable: wikipedia_for_solr\nOwner: jovyan\nCreated Time: Mon May 24 04:10:12 UTC 2021\nLast Access: UNKNOWN\nCreated By: Spark 3.1.1\nType: MANAGED\nProvider: delta\nLocation: file:/home/jovyan/work/wikipedia_in_spark/notebooks/spark-warehouse/wikipedia_for_solr\nSerde Library: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\nInputFormat: org.apache.hadoop.mapred.SequenceFileInputFormat\nOutputFormat: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\nPartition Provider: Catalog)),Some(default.wikipedia_for_solr),None,org.apache.spark.sql.util.CaseInsensitiveStringMap@1f), [ignoreChanges=true], [id#19790, title_txt_en#19791, body_txt_en#19792, person_s#19793, h3_1_s#19794, h3_2_s#19795, tags#19796], org.apache.spark.sql.delta.catalog.DeltaCatalog@450f4666, default.wikipedia_for_solr\n"
     ]
    }
   ],
   "source": [
    "df = spark.readStream.format(\"delta\").option(\"ignoreChanges\",True).table(\"wikipedia_for_solr\")\n",
    "df = spark.readStream.format(\"delta\").option(\"ignoreChanges\",True).load('./tables/try_2_wikipedia_for_solr')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "brazilian-occasions",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "outside-queen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  [\n",
      "    \"529bc3b07127ecb7e53a4dcf1991d9152c24537d919178022b2c42657f79a26b\",\n",
      "    \"\\ufffd\\ufffdIqg\\ufffd6\\ufffd!\\ufffd\\ufffd\\u001c\\u000ft\\ufffdwZ*|`\\u0007=b\\ufffd\\u0004T\\ufffd\",\n",
      "    \"8cde774d6f7333752ed72cacddb05126\",\n",
      "    \"2+q5SXFnjTavIZWFHA90hXdaKnxgBz1i/ARUnA==\"\n",
      "  ]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "data = spark.sql(\"\"\"\n",
    "  SELECT\n",
    "   sha2('Spark',256),\n",
    "   sha2('Spark',224),\n",
    "   md5('Spark'),\n",
    "   base64(sha2('Spark', 224))\n",
    "   ;\n",
    "\"\"\").take(1)\n",
    "print(json.dumps(data,indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
